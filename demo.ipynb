{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bytedeuler opencv-python-headless pillow matplotlib ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import euler\n",
    "\n",
    "euler.install_thrift_import_hook()\n",
    "\n",
    "from idl.base_thrift import *\n",
    "from idl.face_processing_thrift import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "from io import BytesIO\n",
    "from utils import *\n",
    "from prompts import *\n",
    "from videograph import VideoGraph\n",
    "from matplotlib import pyplot as plt\n",
    "import ffmpeg\n",
    "import bytedtos\n",
    "import hashlib\n",
    "import string\n",
    "import random\n",
    "import tempfile\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build client\n",
    "# test_client = euler.Client(FaceService, 'tcp://127.0.0.1:8910', timeout=300, transport='buffered')\n",
    "test_client = euler.Client(\n",
    "    FaceService,\n",
    "    \"sd://lab.agent.face_processing_test?idc=maliva&cluster=default\",\n",
    "    timeout=300,\n",
    "    transport=\"buffered\",\n",
    ")\n",
    "\n",
    "CLUSTER_SIZE = 100\n",
    "\n",
    "# os.environ['CONSUL_HTTP_HOST'] = \"10.54.129.29\"\n",
    "# os.environ['CONSUL_HTTP_PORT'] = 2280\n",
    "# PSM、Cluster、Idc、Accesskey 和 Bucket 可在 TOS 用户平台 > Bucket 详情 > 概览页中查找。具体查询方式详见方式二：通过 “psm+idc” 访问 TOS 桶 。\n",
    "\n",
    "server = \"va\"\n",
    "if server == \"cn\":\n",
    "    ak = \"YFPD6L54IEAAU421YMSG\"\n",
    "    bucket_name = \"vlm-agent\"\n",
    "    tos_psm = \"toutiao.tos.tosapi\"\n",
    "    tos_cluster = \"default\"\n",
    "    tos_idc = \"lf\"\n",
    "    base_url = \"https://tosv.byted.org/obj/vlm-agent/\"\n",
    "elif server == \"va\":\n",
    "    ak = \"BX2M82TQJ7UVTYXYO19Z\"\n",
    "    bucket_name = \"vlm-agent-benchmarking-us\"\n",
    "    tos_psm = \"toutiao.tos.tosapi\"\n",
    "    tos_cluster = \"default\"\n",
    "    tos_idc = \"maliva\"\n",
    "    base_url = \"https://tosv-va.tiktok-row.org/obj/vlm-agent-benchmarking-us/\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid server: {server}\")\n",
    "\n",
    "tos_client = bytedtos.Client(\n",
    "    bucket_name, ak, service=tos_psm, cluster=tos_cluster, idc=tos_idc\n",
    ")\n",
    "\n",
    "\n",
    "def get_hash_key(text):\n",
    "    md5_hash = hashlib.md5()\n",
    "    md5_hash.update(text.encode(\"utf-8\"))\n",
    "    hash_int = int.from_bytes(md5_hash.digest(), byteorder=\"big\")\n",
    "    return abs(hash_int) % (10**8)\n",
    "\n",
    "\n",
    "def generate_random_clip_name(length=10):\n",
    "    return \"\".join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "\n",
    "def upload_one_sample(file, do_upload=True):\n",
    "    try:\n",
    "        obj_key = generate_random_clip_name()\n",
    "        obj_url = base_url + obj_key\n",
    "        if do_upload:\n",
    "            content = open(file, \"rb\")\n",
    "            resp = tos_client.put_object(obj_key, content)\n",
    "            resp_code = int(resp.status_code)\n",
    "            if resp_code != 200:\n",
    "                print(f\"Upoload error code: {resp_code}\")\n",
    "                return -1, \"\"\n",
    "    except bytedtos.TosException as e:\n",
    "        print(\n",
    "            \"Upload failed. code: {}, request_id: {}, message: {}\".format(\n",
    "                e.code, e.request_id, e.msg\n",
    "            )\n",
    "        )\n",
    "        return -1, \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Other error: {e}\")\n",
    "        return -1, \"\"\n",
    "    return obj_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(video_path):\n",
    "    video_info = {}\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened():\n",
    "        raise ValueError(\"Could not open video file\")\n",
    "    video_info[\"fps\"] = video.get(cv2.CAP_PROP_FPS)\n",
    "    video_info[\"frames\"] = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_info[\"duration\"] = video_info[\"frames\"] / video_info[\"fps\"]\n",
    "    video_info[\"path\"] = video_path\n",
    "    video_info[\"name\"] = video_path.split(\"/\")[-1]\n",
    "    video_info[\"width\"] = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_info[\"height\"] = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video_info[\"codec\"] = video.get(cv2.CAP_PROP_CODEC_PIXEL_FORMAT)\n",
    "    video_info[\"format\"] = video.get(cv2.CAP_PROP_FORMAT)\n",
    "    video_info[\"fourcc\"] = video.get(cv2.CAP_PROP_FOURCC)\n",
    "    video.release()\n",
    "\n",
    "    return video_info\n",
    "\n",
    "\n",
    "def extract_frames(video_path, start_time=None, interval=None, sample_fps=10):\n",
    "    video_info = get_video_info(video_path)\n",
    "    # if start_time and interval are not provided, sample the whole video at sample_fps\n",
    "    if start_time is None and interval is None:\n",
    "        start_time = 0\n",
    "        interval = video_info[\"duration\"]\n",
    "    video_fps = video_info[\"fps\"]\n",
    "    total_frames = video_info[\"frames\"]\n",
    "    frame_interval = int(video_fps / sample_fps)\n",
    "\n",
    "    frames = []\n",
    "    segment_video = cv2.VideoCapture(video_path)\n",
    "    segment_video.set(cv2.CAP_PROP_POS_FRAMES, int(start_time * video_fps))\n",
    "    end_frame = min(int((start_time + interval) * video_fps), total_frames)\n",
    "\n",
    "    for frame_idx in range(int(start_time * video_fps), end_frame, frame_interval):\n",
    "        segment_video.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = segment_video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "    segment_video.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def process_video_clip(video_path, start_time, interval, fps):\n",
    "    try:\n",
    "        base64_data = {}\n",
    "        # Create temporary files\n",
    "        temp_files = {\n",
    "            \"video\": tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\"),\n",
    "            \"audio\": tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\"),\n",
    "        }\n",
    "        temp_paths = {k: f.name for k, f in temp_files.items()}\n",
    "        for f in temp_files.values():\n",
    "            f.close()\n",
    "\n",
    "        # Extract video segment\n",
    "        stream = ffmpeg.input(video_path, ss=start_time, t=interval)\n",
    "        stream = ffmpeg.output(\n",
    "            stream, temp_paths[\"video\"], format=\"mp4\", acodec=\"aac\", vcodec=\"libx264\"\n",
    "        )\n",
    "        ffmpeg.run(stream, overwrite_output=True, quiet=True)\n",
    "\n",
    "        # Extract audio\n",
    "        audio_stream = ffmpeg.input(temp_paths[\"video\"])\n",
    "        audio_stream = ffmpeg.output(\n",
    "            audio_stream, temp_paths[\"audio\"], acodec=\"libmp3lame\"\n",
    "        )\n",
    "        ffmpeg.run(audio_stream, overwrite_output=True, quiet=True)\n",
    "\n",
    "        # Read files and convert to Base64\n",
    "        for key, path in temp_paths.items():\n",
    "            with open(path, \"rb\") as f:\n",
    "                base64_data[key] = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "            os.remove(path)\n",
    "\n",
    "        base64_data[\"frames\"] = extract_frames(video_path, start_time, interval, fps)\n",
    "\n",
    "        return base64_data[\"video\"], base64_data[\"frames\"], base64_data[\"audio\"]\n",
    "\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"FFmpeg Error:\", e.stderr.decode())\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the given faces are recognizable\n",
    "def batch_classify_faces(faces):\n",
    "    print(len(faces))\n",
    "    base64_faces = [face[\"extra_data\"][\"face_base64\"] for face in faces]\n",
    "    inputs = [\n",
    "        [\n",
    "            {\"type\": \"images\", \"content\": [base64_face]},\n",
    "            {\"type\": \"text\", \"content\": prompt_classify_recognizable_faces},\n",
    "        ]\n",
    "        for base64_face in base64_faces\n",
    "    ]\n",
    "    messages = [generate_messages(input) for input in inputs]\n",
    "    model = \"gemini-1.5-pro-002\"\n",
    "    response = parallel_get_response(model, messages)\n",
    "    for i in range(len(response[0])):\n",
    "        faces[i][\"extra_data\"][\"recognizable\"] = int(response[0][i])\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_representative_faces_with_rules(faces):\n",
    "    \"\"\"Select the most representative face for each cluster based on face type, size and similarity.\n",
    "\n",
    "    Args:\n",
    "        faces (list): List of face dictionaries containing frame_id, bounding_box, face_emb,\n",
    "                     cluster_id and extra_data with face_type\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping of cluster_id to the most representative face\n",
    "    \"\"\"\n",
    "    # Group faces by cluster\n",
    "    clusters = {}\n",
    "    for face in faces:\n",
    "        cluster_id = face[\"cluster_id\"]\n",
    "        if cluster_id == -1:  # Skip noise points\n",
    "            continue\n",
    "        if cluster_id not in clusters:\n",
    "            clusters[cluster_id] = []\n",
    "        clusters[cluster_id].append(face)\n",
    "\n",
    "    representative_faces = {}\n",
    "\n",
    "    # For each cluster, find the best representative face\n",
    "    for cluster_id, cluster_faces in clusters.items():\n",
    "        # First try to find ortho faces\n",
    "        ortho_faces = []\n",
    "        side_faces = []\n",
    "        for f in cluster_faces:\n",
    "            if f[\"extra_data\"][\"face_type\"] == \"ortho\":\n",
    "                ortho_faces.append(f)\n",
    "            else:\n",
    "                side_faces.append(f)\n",
    "\n",
    "        if ortho_faces:\n",
    "            # For ortho faces, first select top 10% by size\n",
    "            areas = [\n",
    "                (\n",
    "                    f,\n",
    "                    (f[\"bounding_box\"][2] - f[\"bounding_box\"][0])\n",
    "                    * (f[\"bounding_box\"][3] - f[\"bounding_box\"][1]),\n",
    "                )\n",
    "                for f in ortho_faces\n",
    "            ]\n",
    "            areas.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_size_faces = [f[0] for f in areas[: max(1, int(len(areas) * 0.1))]]\n",
    "\n",
    "            # If only one face remains, use it directly\n",
    "            if len(top_size_faces) == 1:\n",
    "                best_face = top_size_faces[0]\n",
    "            else:\n",
    "                # Find the one with highest average similarity to all faces in cluster\n",
    "                max_avg_similarity = -1\n",
    "                best_face = None\n",
    "                cluster_embeddings = np.array(\n",
    "                    [face[\"face_emb\"] for face in cluster_faces]\n",
    "                )\n",
    "\n",
    "                for face in top_size_faces:\n",
    "                    similarities = np.dot(cluster_embeddings, face[\"face_emb\"])\n",
    "                    avg_similarity = (np.sum(similarities) - 1) / (\n",
    "                        len(cluster_faces) - 1\n",
    "                    )\n",
    "                    if avg_similarity > max_avg_similarity:\n",
    "                        max_avg_similarity = avg_similarity\n",
    "                        best_face = face\n",
    "\n",
    "        else:\n",
    "            # For side faces, first select top 20% by aspect ratio closest to 1\n",
    "            if side_faces:\n",
    "                areas = [\n",
    "                    (\n",
    "                        f,\n",
    "                        (f[\"bounding_box\"][2] - f[\"bounding_box\"][0])\n",
    "                        * (f[\"bounding_box\"][3] - f[\"bounding_box\"][1]),\n",
    "                    )\n",
    "                    for f in side_faces\n",
    "                ]\n",
    "                areas.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_area_faces = [f[0] for f in areas[: max(1, int(len(areas) * 0.5))]]\n",
    "\n",
    "                # Then select top 20% by aspect ratio closest to 1\n",
    "                ratios = []\n",
    "                for face in top_area_faces:\n",
    "                    bbox = face[\"bounding_box\"]\n",
    "                    width = bbox[2] - bbox[0]\n",
    "                    height = bbox[3] - bbox[1]\n",
    "                    ratio = abs(width / height - 1.0)\n",
    "                    ratios.append((face, ratio))\n",
    "\n",
    "                ratios.sort(key=lambda x: x[1])  # Sort by ratio difference from 1\n",
    "                final_candidates = [\n",
    "                    f[0] for f in ratios[: max(1, int(len(ratios) * 0.2))]\n",
    "                ]\n",
    "\n",
    "                # If only one face remains, use it directly\n",
    "                if len(final_candidates) == 1:\n",
    "                    best_face = final_candidates[0]\n",
    "                else:\n",
    "                    # Find the one with highest average similarity to all faces in cluster\n",
    "                    max_avg_similarity = -1\n",
    "                    best_face = None\n",
    "                    cluster_embeddings = np.array(\n",
    "                        [face[\"face_emb\"] for face in cluster_faces]\n",
    "                    )\n",
    "\n",
    "                    for face in final_candidates:\n",
    "                        similarities = np.dot(cluster_embeddings, face[\"face_emb\"])\n",
    "                        avg_similarity = (np.sum(similarities) - 1) / (\n",
    "                            len(cluster_faces) - 1\n",
    "                        )\n",
    "                        if avg_similarity > max_avg_similarity:\n",
    "                            max_avg_similarity = avg_similarity\n",
    "                            best_face = face\n",
    "\n",
    "        representative_faces[cluster_id] = best_face\n",
    "\n",
    "    # return representative_faces\n",
    "\n",
    "    faces_list = []\n",
    "    for cluster_id, face in representative_faces.items():\n",
    "        faces_list.append(face)\n",
    "    return faces_list\n",
    "\n",
    "\n",
    "def select_representative_faces_with_scores(faces, max_faces=3):\n",
    "    # Group faces by cluster\n",
    "    clusters = {}\n",
    "    for face in faces:\n",
    "        cluster_id = face[\"cluster_id\"]\n",
    "        if cluster_id == -1:  # Skip noise points\n",
    "            continue\n",
    "        if cluster_id not in clusters:\n",
    "            clusters[cluster_id] = []\n",
    "        clusters[cluster_id].append(face)\n",
    "\n",
    "    faces_list = []\n",
    "    faces_per_cluster = {}\n",
    "    dthresh = 0.85\n",
    "    qthresh = 22\n",
    "\n",
    "    # For each cluster, find the best representative face\n",
    "    for cluster_id, cluster_faces in clusters.items():\n",
    "        qualified_faces = [\n",
    "            face\n",
    "            for face in cluster_faces\n",
    "            if float(face[\"extra_data\"][\"face_detection_score\"]) > dthresh\n",
    "            and float(face[\"extra_data\"][\"face_quality_score\"]) > qthresh\n",
    "        ]\n",
    "        if qualified_faces:\n",
    "            # Sort faces by face_detection_score and face_quality_score\n",
    "            sorted_faces = sorted(\n",
    "                qualified_faces,\n",
    "                key=lambda x: (\n",
    "                    float(x[\"extra_data\"][\"face_detection_score\"]),\n",
    "                    float(x[\"extra_data\"][\"face_quality_score\"]),\n",
    "                ),\n",
    "                reverse=True,\n",
    "            )\n",
    "            # Select the face with the highest face_detection_score and face_quality_score\n",
    "            best_faces = sorted_faces[:max_faces]\n",
    "            faces_per_cluster[cluster_id] = best_faces\n",
    "            faces_list.append(best_faces)\n",
    "\n",
    "    return faces_list\n",
    "    # return faces_per_cluster\n",
    "\n",
    "\n",
    "def select_representative_faces_with_gpt(faces):\n",
    "    # Group faces by cluster\n",
    "    clusters = {}\n",
    "    for face in faces:\n",
    "        cluster_id = face[\"cluster_id\"]\n",
    "        if cluster_id == -1:  # Skip noise points\n",
    "            continue\n",
    "        if cluster_id not in clusters:\n",
    "            clusters[cluster_id] = []\n",
    "        clusters[cluster_id].append(face)\n",
    "\n",
    "    faces_list = []\n",
    "\n",
    "    # For each cluster, find the best representative face\n",
    "    for cluster_id, cluster_faces in clusters.items():\n",
    "        faces_base64 = [face[\"extra_data\"][\"face_base64\"] for face in cluster_faces]\n",
    "        print(f\"faces number: {len(faces_base64)}\")\n",
    "        # model = 'gemini-1.5-pro-002'\n",
    "        # input = [\n",
    "        #     {\n",
    "        #         \"type\": \"images\",\n",
    "        #         \"content\": faces_base64,\n",
    "        #     },\n",
    "        #     {\n",
    "        #         \"type\": \"text\",\n",
    "        #         \"content\": prompt_select_representative_faces,\n",
    "        #     }\n",
    "        # ]\n",
    "        model = \"gpt-4o-2024-11-20\"\n",
    "        input = [\n",
    "            {\n",
    "                \"type\": \"images\",\n",
    "                \"content\": faces_base64,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"content\": prompt_select_representative_faces_forced,\n",
    "            },\n",
    "        ]\n",
    "        messages = generate_messages(input)\n",
    "\n",
    "        response = get_response_with_retry(model, messages)\n",
    "        try:\n",
    "            index = int(response[0])\n",
    "            if index >= 0:\n",
    "                print(f\"best face: {index}\")\n",
    "                faces_list.append(cluster_faces[index])\n",
    "            else:\n",
    "                print(f\"cannot find a good face\")\n",
    "                # insert a face with black base64\n",
    "                size = (100, 100)\n",
    "                black_image = Image.new(\"RGB\", size, (0, 0, 0))\n",
    "                buffered = BytesIO()\n",
    "                black_image.save(buffered, format=\"JPEG\")\n",
    "                img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "                black_face = {\n",
    "                    \"frame_id\": -1,\n",
    "                    \"bounding_box\": [0, 0, 0, 0],\n",
    "                    \"face_emb\": np.zeros_like(cluster_faces[0][\"face_emb\"]).tolist(),\n",
    "                    \"cluster_id\": -1,\n",
    "                    \"extra_data\": {\"face_type\": \"other\", \"face_base64\": img_base64},\n",
    "                }\n",
    "                faces_list.append(black_face)\n",
    "        except:\n",
    "            print(f\"cannot find a good face\")\n",
    "            # insert a face with black base64\n",
    "            size = (100, 100)\n",
    "            black_image = Image.new(\"RGB\", size, (0, 0, 0))\n",
    "            buffered = BytesIO()\n",
    "            black_image.save(buffered, format=\"JPEG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            black_face = {\n",
    "                \"frame_id\": -1,\n",
    "                \"bounding_box\": [0, 0, 0, 0],\n",
    "                \"face_emb\": np.zeros_like(cluster_faces[0][\"face_emb\"]).tolist(),\n",
    "                \"cluster_id\": -1,\n",
    "                \"extra_data\": {\"face_type\": \"other\", \"face_base64\": img_base64},\n",
    "            }\n",
    "            faces_list.append(black_face)\n",
    "\n",
    "    return faces_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thinkings_with_ids(video_context, video_description):\n",
    "    input = video_context + [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"content\": f\"video_description: {video_description}\",\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"content\": prompt_generate_thinkings_with_ids,\n",
    "        },\n",
    "    ]\n",
    "    messages = generate_messages(input)\n",
    "    model = \"gemini-1.5-pro-002\"\n",
    "    response = get_response_with_retry(model, messages)\n",
    "\n",
    "    print(\"*\" * 20)\n",
    "    print(\"Video Thinkings:\")\n",
    "    print(response[0])\n",
    "\n",
    "    return response[0]\n",
    "\n",
    "\n",
    "def generate_captions_and_thinkings_with_ids(\n",
    "    base64_video, base64_frames, base64_audio, faces_list, voices_list\n",
    "):\n",
    "    face_frames = []\n",
    "\n",
    "    print(f\"id num: {len(faces_list)}\")\n",
    "    # print(len(faces_list[0]))\n",
    "\n",
    "    # Iterate through faces directly\n",
    "    for char_id, faces in faces_list.items():\n",
    "        face = faces[0]\n",
    "        frame_id = face[\"frame_id\"]\n",
    "        frame_base64 = base64_frames[frame_id]\n",
    "\n",
    "        # Convert base64 to PIL Image\n",
    "        frame_bytes = base64.b64decode(frame_base64)\n",
    "        frame_img = Image.open(BytesIO(frame_bytes))\n",
    "        draw = ImageDraw.Draw(frame_img)\n",
    "\n",
    "        # Draw current face\n",
    "        bbox = face[\"bounding_box\"]\n",
    "        draw.rectangle(\n",
    "            [(bbox[0], bbox[1]), (bbox[2], bbox[3])], outline=(0, 255, 0), width=4\n",
    "        )\n",
    "\n",
    "        # Convert back to base64\n",
    "        buffered = BytesIO()\n",
    "        frame_img.save(buffered, format=\"JPEG\")\n",
    "        frame_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        face_frames.append((f\"<char_{char_id}>:\", frame_base64))\n",
    "\n",
    "    # print(video_url)\n",
    "    print(len(base64_video))\n",
    "    video_context = [\n",
    "        {\n",
    "            \"type\": \"video_base64\",\n",
    "            \"content\": base64_video,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"images\",\n",
    "            \"content\": face_frames,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"content\": voices_list,\n",
    "        },\n",
    "    ]\n",
    "    input = video_context + [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"content\": prompt_generate_captions_with_ids,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = generate_messages(input)\n",
    "    model = \"gemini-1.5-pro-002\"\n",
    "    captions = get_response_with_retry(model, messages)\n",
    "\n",
    "    # Visualize face frames with IDs\n",
    "    num_faces = len(face_frames)\n",
    "    num_rows = (num_faces + 2) // 3  # Round up division to get number of rows needed\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(15, 5 * num_rows))\n",
    "    axes = axes.ravel()  # Flatten axes array for easier indexing\n",
    "\n",
    "    for i, face_frame in enumerate(face_frames):\n",
    "        # Convert base64 to image array\n",
    "        img_bytes = base64.b64decode(face_frame[1])\n",
    "        img_array = np.array(Image.open(BytesIO(img_bytes)))\n",
    "\n",
    "        axes[i].imshow(img_array)\n",
    "        axes[i].set_title(face_frame[0])\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(voices_list)\n",
    "\n",
    "    # Print response\n",
    "    print(\"*\" * 20)\n",
    "    print(\"Video Descriptions:\")\n",
    "    print(captions[0])\n",
    "\n",
    "    thinkings = generate_thinkings_with_ids(video_context, captions[0])\n",
    "\n",
    "    return captions[0], thinkings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_descriptions(video_graph, video_descriptions_string):\n",
    "    def string_to_list(s):\n",
    "        try:\n",
    "            # Remove ```json or ``` from start/end\n",
    "            s = s.strip(\"```json\")\n",
    "            s = s.strip(\"```\")\n",
    "            result = ast.literal_eval(s)\n",
    "            if isinstance(result, list):\n",
    "                return result\n",
    "            else:\n",
    "                raise ValueError(\"Input string is not a list\")\n",
    "        except (SyntaxError, ValueError) as e:\n",
    "            print(f\"Parsing error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_video_description(video_description):\n",
    "        # video_description is a string like this: <char_1> xxx <char_2> xxx\n",
    "        # extract all the elements wrapped by < and >\n",
    "        entities = []\n",
    "        current_entity = \"\"\n",
    "        in_entity = False\n",
    "\n",
    "        for char in video_description:\n",
    "            if char == \"<\":\n",
    "                in_entity = True\n",
    "                current_entity = \"\"\n",
    "            elif char == \">\":\n",
    "                in_entity = False\n",
    "                node_type, node_id = current_entity.split(\"_\")\n",
    "                # TODO: check node_id dtype\n",
    "                entities.append((node_type, node_id))\n",
    "            else:\n",
    "                if in_entity:\n",
    "                    current_entity += char\n",
    "        return entities\n",
    "\n",
    "    def update_video_graph(video_graph, descriptions):\n",
    "        for description in descriptions:\n",
    "            new_node_id = video_graph.add_text_node(description)\n",
    "            entities = parse_video_description(description)\n",
    "            for _, node_id in entities:\n",
    "                video_graph.add_edge(new_node_id, node_id)\n",
    "\n",
    "    descriptions = string_to_list(video_descriptions_string)\n",
    "    print(descriptions)\n",
    "    update_video_graph(video_graph, descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(params):\n",
    "    frames = params[0]\n",
    "    offset = params[1]\n",
    "    req = SingleGetFaceRequest(frames=frames, Base=Base())\n",
    "    resp = test_client.SingleGetFace(req)\n",
    "    faces = resp.faces\n",
    "    for face in faces:\n",
    "        face.frame_id += offset\n",
    "    return faces\n",
    "\n",
    "\n",
    "def process_faces(video_graph, base64_frames, batch_size):\n",
    "    def get_embeddings(base64_frames, batch_size):\n",
    "        num_batches = (len(base64_frames) + batch_size - 1) // batch_size\n",
    "        batched_frames = [\n",
    "            (base64_frames[i * batch_size : (i + 1) * batch_size], i * batch_size)\n",
    "            for i in range(num_batches)\n",
    "        ]\n",
    "\n",
    "        faces = []\n",
    "\n",
    "        # parallel process the batches\n",
    "        with ThreadPoolExecutor(max_workers=num_batches) as executor:\n",
    "            for batch_faces in tqdm(\n",
    "                executor.map(process_batch, batched_frames), total=num_batches\n",
    "            ):\n",
    "                faces.extend(batch_faces)\n",
    "\n",
    "        req = SingleClusterFaceRequest(faces=faces, Base=Base())\n",
    "        resp = test_client.SingleClusterFace(req)\n",
    "\n",
    "        faces = resp.faces\n",
    "\n",
    "        return faces\n",
    "\n",
    "    def establish_mapping(faces, key=\"cluster_id\"):\n",
    "        mapping = {}\n",
    "        if key in faces[0].keys():\n",
    "            for face in faces:\n",
    "                key = face[key]\n",
    "                if key not in mapping:\n",
    "                    mapping[key] = []\n",
    "                mapping[key].append(face)\n",
    "        else:\n",
    "            raise ValueError(f\"key {key} not found in faces\")\n",
    "        # sort the faces in each cluster by detection score and quality score\n",
    "        for key in mapping:\n",
    "            mapping[key] = sorted(\n",
    "                mapping[key],\n",
    "                key=lambda x: (\n",
    "                    float(x[\"extra_data\"][\"face_detection_score\"]),\n",
    "                    float(x[\"extra_data\"][\"face_quality_score\"]),\n",
    "                ),\n",
    "                reverse=True,\n",
    "            )\n",
    "        return mapping\n",
    "\n",
    "    def filter_score_based(faces):\n",
    "        dthresh = 0.85\n",
    "        qthresh = 22\n",
    "        max_faces = 3\n",
    "        filtered_faces = [\n",
    "            face\n",
    "            for face in faces\n",
    "            if float(face[\"extra_data\"][\"face_detection_score\"]) > dthresh\n",
    "            and float(face[\"extra_data\"][\"face_quality_score\"]) > qthresh\n",
    "        ]\n",
    "        return filtered_faces[:max_faces]\n",
    "\n",
    "    def update_videograph(video_graph, tempid2faces, filter=None):\n",
    "        faces_list = []\n",
    "        for tempid, faces in tempid2faces.items():\n",
    "            if tempid == -1:\n",
    "                continue\n",
    "            if filter:\n",
    "                filtered_faces = filter(faces)\n",
    "            else:\n",
    "                filtered_faces = faces\n",
    "            face_embs = [face[\"face_emb\"] for face in filtered_faces]\n",
    "            matched_nodes = video_graph.search_img_nodes(face_embs)\n",
    "            if len(matched_nodes) > 0:\n",
    "                matched_node = matched_nodes[0]\n",
    "                video_graph.add_embedding(matched_node, face_embs)\n",
    "                for face in faces:\n",
    "                    face[\"matched_node\"] = matched_node\n",
    "            else:\n",
    "                matched_node = video_graph.add_img_node(face_embs)\n",
    "                for face in faces:\n",
    "                    face[\"matched_node\"] = matched_node\n",
    "            faces_list.extend(filtered_faces)\n",
    "\n",
    "        return faces_list\n",
    "\n",
    "    faces = get_embeddings(base64_frames, batch_size)\n",
    "\n",
    "    faces_json = [\n",
    "        {\n",
    "            \"frame_id\": face.frame_id,\n",
    "            \"bounding_box\": face.bounding_box,\n",
    "            \"face_emb\": face.face_emb,\n",
    "            \"cluster_id\": face.cluster_id,\n",
    "            \"extra_data\": face.extra_data,\n",
    "        }\n",
    "        for face in faces\n",
    "    ]\n",
    "\n",
    "    tempid2faces = establish_mapping(faces_json, key=\"cluster_id\")\n",
    "\n",
    "    tagged_faces_json = update_videograph(\n",
    "        video_graph, tempid2faces, filter=filter_score_based\n",
    "    )\n",
    "\n",
    "    id2faces = establish_mapping(tagged_faces_json, key=\"matched_node\")\n",
    "\n",
    "    return id2faces\n",
    "\n",
    "\n",
    "def process_voices(base64_audio):\n",
    "    input = [\n",
    "        {\n",
    "            \"type\": \"audio\",\n",
    "            \"content\": base64_audio,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"content\": prompt_audio_diarization,\n",
    "        },\n",
    "    ]\n",
    "    messages = generate_messages(input)\n",
    "    model = \"gemini-1.5-pro-002\"\n",
    "    response = get_response_with_retry(model, messages)\n",
    "    return response[0]\n",
    "\n",
    "\n",
    "def process_segment(video_graph, base64_video, base64_frames, base64_audio, batch_size):\n",
    "\n",
    "    print(f\"processing {len(base64_frames)} frames...\")\n",
    "\n",
    "    id2faces = process_faces(video_graph, base64_frames, batch_size)\n",
    "    print(\"Finish processing faces\")\n",
    "\n",
    "    id2voices = process_voices(video_graph, base64_audio)\n",
    "    print(\"Finish processing voices\")\n",
    "\n",
    "    captions, thinkings = generate_captions_and_thinkings_with_ids(\n",
    "        base64_video,\n",
    "        base64_frames,\n",
    "        base64_audio,\n",
    "        id2faces,\n",
    "        id2voices,\n",
    "    )\n",
    "\n",
    "    process_descriptions(video_graph, captions)\n",
    "    process_descriptions(video_graph, thinkings)\n",
    "\n",
    "    print(\"Finish processing segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = ''\n",
    "# base64_list = extract_frames(video_path)\n",
    "# print(len(base64_list))\n",
    "\n",
    "# req = SingleGetFaceRequest(frames=base64_list, Base=Base())\n",
    "# resp = test_client.SingleGetFace(req)\n",
    "# print(resp)\n",
    "\n",
    "# faces = resp.faces\n",
    "# req = SingleClusterFaceRequest(faces=faces, Base=Base())\n",
    "# resp = test_client.SingleClusterFace(req)\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_process_video(\n",
    "    video_graph, video_path, interval_seconds, fps, segment_limit=None\n",
    "):\n",
    "    \"\"\"Process video segments at specified intervals with given fps.\n",
    "\n",
    "    Args:\n",
    "        video_graph (VideoGraph): Graph object to store video information\n",
    "        video_path (str): Path to the video file\n",
    "        interval_seconds (float): Time interval between segments in seconds\n",
    "        fps (float): Frames per second to extract from each segment\n",
    "\n",
    "    Returns:\n",
    "        None: Updates video_graph in place with processed segments\n",
    "    \"\"\"\n",
    "\n",
    "    video_info = get_video_info(video_path)\n",
    "    print(video_info)\n",
    "\n",
    "    # Process each interval\n",
    "    count = 0\n",
    "    for start_time in np.arange(0, video_info[\"duration\"], interval_seconds):\n",
    "\n",
    "        base64_video, base64_frames, base64_audio = process_video_clip(\n",
    "            video_path, start_time, interval_seconds, fps\n",
    "        )\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        # Process frames for this interval\n",
    "        if base64_frames:\n",
    "            print(\"=\" * 20)\n",
    "            print(\n",
    "                f\"Starting processing {count}-th clips starting at {start_time} seconds...\"\n",
    "            )\n",
    "            process_segment(\n",
    "                video_graph,\n",
    "                base64_video,\n",
    "                base64_frames,\n",
    "                base64_audio,\n",
    "                interval_seconds * fps // (CLUSTER_SIZE),\n",
    "            )\n",
    "\n",
    "        if segment_limit is not None:\n",
    "            if count >= segment_limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_graph = VideoGraph()\n",
    "video_path = \"/mnt/bn/videonasi18n/longlin.kylin/vlm-agent-benchmarking/data/videos/raw/720p/5 Poor People vs 1 Secret Millionaire.mp4\"\n",
    "\n",
    "streaming_process_video(\n",
    "    video_graph, video_path, interval_seconds=30, fps=10, segment_limit=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "fileId": "9e153c75-6630-46b6-af1a-76d1a8277f1c",
  "filePath": "/mnt/bn/videonasi18n/longlin.kylin/tce-face-extraction/demo.ipynb",
  "kernelspec": {
   "display_name": "vlm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
