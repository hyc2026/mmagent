{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "from videograph import VideoGraph\n",
    "from utils.general import *\n",
    "from utils.video_processing import *\n",
    "from utils.chat_api import *\n",
    "from prompts import *\n",
    "\n",
    "from face_processing import process_faces\n",
    "from voice_processing import process_voices\n",
    "from memory_processing import (\n",
    "    process_captions,\n",
    "    generate_captions_and_thinkings_with_ids,\n",
    ")\n",
    "from retrieve import answer_with_retrieval\n",
    "\n",
    "processing_config = json.load(open(\"configs/processing_config.json\"))\n",
    "memory_config = json.load(open(\"configs/memory_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(video_graph, base64_video, base64_frames, base64_audio, clip_id):\n",
    "\n",
    "    id2voices = process_voices(video_graph, base64_audio, base64_video)\n",
    "    print(\"Finish processing voices\")\n",
    "\n",
    "    id2faces = process_faces(video_graph, base64_frames)\n",
    "    print(\"Finish processing faces\")\n",
    "\n",
    "    episodic_captions, semantic_captions = generate_captions_and_thinkings_with_ids(\n",
    "        video_graph,\n",
    "        base64_video,\n",
    "        base64_frames,\n",
    "        id2faces,\n",
    "        id2voices,\n",
    "        clip_id,\n",
    "    )\n",
    "\n",
    "    process_captions(video_graph, episodic_captions, clip_id, type=\"episodic\")\n",
    "    process_captions(video_graph, semantic_captions, clip_id, type=\"semantic\")\n",
    "\n",
    "    print(\"Finish processing segment\")\n",
    "\n",
    "\n",
    "def streaming_process_video(video_graph, video_path):\n",
    "    \"\"\"Process video segments at specified intervals with given fps.\n",
    "\n",
    "    Args:\n",
    "        video_graph (VideoGraph): Graph object to store video information\n",
    "        video_path (str): Path to the video file or directory containing clips\n",
    "        interval_seconds (float): Time interval between segments in seconds\n",
    "        fps (float): Frames per second to extract from each segment\n",
    "\n",
    "    Returns:\n",
    "        None: Updates video_graph in place with processed segments\n",
    "    \"\"\"\n",
    "    interval_seconds = processing_config[\"interval_seconds\"]\n",
    "    fps = processing_config[\"fps\"]\n",
    "    segment_limit = processing_config[\"segment_limit\"]\n",
    "\n",
    "    if os.path.isfile(video_path):\n",
    "        # Process single video file\n",
    "        video_info = get_video_info(video_path)\n",
    "\n",
    "        # Process each interval\n",
    "        count = 0\n",
    "        for start_time in tqdm(np.arange(0, video_info[\"duration\"], interval_seconds)):\n",
    "            if start_time + interval_seconds > video_info[\"duration\"]:\n",
    "                break\n",
    "\n",
    "            print(\"=\" * 20)\n",
    "\n",
    "            print(f\"Loading {count}-th clip starting at {start_time} seconds...\")\n",
    "            base64_video, base64_frames, base64_audio = process_video_clip(\n",
    "                video_path, start_time, interval_seconds, fps, audio_format=\"wav\"\n",
    "            )\n",
    "\n",
    "            # Process frames for this interval\n",
    "            if base64_frames:\n",
    "                print(\n",
    "                    f\"Starting processing {count}-th clip starting at {start_time} seconds...\"\n",
    "                )\n",
    "                process_segment(\n",
    "                    video_graph, base64_video, base64_frames, base64_audio, count\n",
    "                )\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if segment_limit > 0 and count >= segment_limit:\n",
    "                break\n",
    "\n",
    "    elif os.path.isdir(video_path):\n",
    "        # Process directory of numbered clips\n",
    "        files = os.listdir(video_path)\n",
    "        # Filter for video files and sort by numeric value in filename\n",
    "        video_files = [\n",
    "            f for f in files if any(f.endswith(ext) for ext in [\".mp4\", \".avi\", \".mov\"])\n",
    "        ]\n",
    "        video_files.sort(key=lambda x: int(\"\".join(filter(str.isdigit, x))))\n",
    "\n",
    "        for count, video_file in enumerate(tqdm(video_files)):\n",
    "            if segment_limit > 0 and count >= segment_limit:\n",
    "                break\n",
    "            print(\"=\" * 20)\n",
    "            full_path = os.path.join(video_path, video_file)\n",
    "            print(f\"Starting processing {count}-th clip: {full_path}\")\n",
    "\n",
    "            base64_video, base64_frames, base64_audio = process_video_clip(\n",
    "                full_path, 0, None, fps, audio_format=\"wav\"\n",
    "            )\n",
    "\n",
    "            if base64_frames:\n",
    "                process_segment(\n",
    "                    video_graph, base64_video, base64_frames, base64_audio, count\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video paths can be paths to directories or paths to mp4 files\n",
    "video_paths = processing_config[\"video_paths\"]\n",
    "\n",
    "for video_path in video_paths:\n",
    "\n",
    "    video_graph = VideoGraph(**memory_config)\n",
    "\n",
    "    streaming_process_video(video_graph, video_path)\n",
    "\n",
    "    video_graph.refresh_equivalences()\n",
    "\n",
    "    save_dir = processing_config[\"save_dir\"]\n",
    "    save_video_graph(\n",
    "        video_graph,\n",
    "        video_path,\n",
    "        save_dir,\n",
    "        (processing_config, memory_config),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/utils/chat_api.py:135\u001b[0m, in \u001b[0;36mparallel_get_embedding\u001b[0;34m(model, texts)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mmax_workers) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m--> 135\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(executor\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m x: get_embedding_with_retry(model, x), texts))\n\u001b[1;32m    137\u001b[0m httpx_logger\u001b[39m.\u001b[39msetLevel(original_log_level)\n",
      "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:600\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[39myield\u001b[39;00m fs\u001b[39m.\u001b[39;49mpop()\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    601\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:435\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 435\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m     gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mWhat does Demar Randy wear?\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# question = \"Who has an OnlyFans account?\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# question = \"Is Stewart Thompson a person who pursues a high-quality life?\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# question = \"What is the rule of the game?\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m answer \u001b[39m=\u001b[39m answer_with_retrieval(\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     video_graph,\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     question,\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     query_num\u001b[39m=\u001b[39;49mprocessing_config[\u001b[39m\"\u001b[39;49m\u001b[39mquery_num\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     topk\u001b[39m=\u001b[39;49mprocessing_config[\u001b[39m\"\u001b[39;49m\u001b[39mtopk\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# video_graph.summarize(logging=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# save_dir = \"data/video_graphs\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# save_video_graph(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#     video_graph, None, save_dir, None, file_name='5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75_augmented.pkl'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# video_graph.visualize()\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/retrieve.py:170\u001b[0m, in \u001b[0;36manswer_with_retrieval\u001b[0;34m(video_graph, question, query_num, topk, auto_refresh)\u001b[0m\n\u001b[1;32m    167\u001b[0m related_memories \u001b[39m=\u001b[39m {}\n\u001b[1;32m    169\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_retrieval_steps):\n\u001b[0;32m--> 170\u001b[0m     new_clips \u001b[39m=\u001b[39m retrieve_from_videograph(video_graph, question, related_memories, query_num, topk)\n\u001b[1;32m    171\u001b[0m     new_clips \u001b[39m=\u001b[39m [new_clip \u001b[39mfor\u001b[39;00m new_clip \u001b[39min\u001b[39;00m new_clips \u001b[39mif\u001b[39;00m new_clip \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m related_clips]\n\u001b[1;32m    172\u001b[0m     related_clips\u001b[39m.\u001b[39mextend(new_clips)\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/retrieve.py:142\u001b[0m, in \u001b[0;36mretrieve_from_videograph\u001b[0;34m(video_graph, question, related_memories, query_num, topk)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQueries: \u001b[39m\u001b[39m{\u001b[39;00mqueries\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-embedding-3-large\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 142\u001b[0m query_embeddings \u001b[39m=\u001b[39m parallel_get_embedding(model, queries)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    144\u001b[0m clip_scores \u001b[39m=\u001b[39m {}\n\u001b[1;32m    146\u001b[0m \u001b[39mfor\u001b[39;00m query_embedding \u001b[39min\u001b[39;00m query_embeddings:\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/utils/chat_api.py:135\u001b[0m, in \u001b[0;36mparallel_get_embedding\u001b[0;34m(model, texts)\u001b[0m\n\u001b[1;32m    131\u001b[0m httpx_logger\u001b[39m.\u001b[39msetLevel(logging\u001b[39m.\u001b[39mCRITICAL)\n\u001b[1;32m    134\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mmax_workers) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m--> 135\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(executor\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: get_embedding_with_retry(model, x), texts))\n\u001b[1;32m    137\u001b[0m httpx_logger\u001b[39m.\u001b[39msetLevel(original_log_level)\n\u001b[1;32m    139\u001b[0m \u001b[39m# Split results into embeddings and tokens\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:628\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 628\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshutdown(wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/thread.py:229\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m    228\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads:\n\u001b[0;32m--> 229\u001b[0m         t\u001b[39m.\u001b[39;49mjoin()\n",
      "File \u001b[0;32m/usr/lib/python3.9/threading.py:1033\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1034\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.9/threading.py:1049\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m lock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1049\u001b[0m \u001b[39melif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1050\u001b[0m     lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1051\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_graph_path = \"data/mems/5-Poor-People-vs-1-Secret-Millionaire_60_5_-1_10_20_0.3_0.6.pkl\"\n",
    "video_graph = load_video_graph(video_graph_path)\n",
    "# for text_node in video_graph.text_nodes:\n",
    "#     print(video_graph.nodes[text_node].metadata['contents'])\n",
    "# for nodes, weight in video_graph.edges.items():\n",
    "#     if weight > 1:\n",
    "#         if video_graph.nodes[nodes[0]].type in [\"episodic\", \"semantic\"]:\n",
    "#            print(video_graph.nodes[nodes[0]].metadata['contents'])\n",
    "#         else:\n",
    "#            print(video_graph.nodes[nodes[1]].metadata['contents'])\n",
    "#         print(weight)\n",
    "\n",
    "# video_graph.refresh_equivalences()\n",
    "\n",
    "question = 'What does Demar Randy wear?'\n",
    "# question = \"Who has an OnlyFans account?\"\n",
    "# question = \"Is Stewart Thompson a person who pursues a high-quality life?\"\n",
    "# question = \"What is the rule of the game?\"\n",
    "answer = answer_with_retrieval(\n",
    "    video_graph,\n",
    "    question,\n",
    "    query_num=processing_config[\"query_num\"],\n",
    "    topk=processing_config[\"topk\"],\n",
    ")\n",
    "\n",
    "# video_graph.summarize(logging=True)\n",
    "# save_dir = \"data/video_graphs\"\n",
    "# save_video_graph(\n",
    "#     video_graph, None, save_dir, None, file_name='5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75_augmented.pkl'\n",
    "# )\n",
    "# video_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chat_api import *\n",
    "from utils.general import plot_cosine_similarity_distribution\n",
    "\n",
    "video_graph_path = \"data/video_graphs/5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75.pkl\"\n",
    "video_graph = load_video_graph(video_graph_path)\n",
    "\n",
    "graph_embeddings = []\n",
    "\n",
    "for id, node in video_graph.nodes.items():\n",
    "    if node.type in [\"episodic\", \"semantic\"]:\n",
    "        graph_embeddings.extend(node.embeddings)\n",
    "\n",
    "# texts = [\"Clothing style of Demar Randy\", \"<voice_44> introduces himself as Demar Randy.\"]\n",
    "texts = [\"<face_4> points at <face_9>.\"]\n",
    "embs = parallel_get_embedding(\"text-embedding-3-large\", texts)[0]\n",
    "\n",
    "plot_cosine_similarity_distribution(graph_embeddings, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_node in video_graph.text_nodes:\n",
    "    print(video_graph.nodes[text_node].metadata[\"contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from retrieve import retrieve_from_videograph\n",
    "# from videograph import VideoGraph\n",
    "# from utils.chat_api import (\n",
    "#     generate_messages,\n",
    "#     get_response_with_retry,\n",
    "#     parallel_get_embedding,\n",
    "# )\n",
    "# from utils.general import validate_and_fix_python_list\n",
    "# from prompts import prompt_memory_retrieval\n",
    "\n",
    "# MAX_RETRIES = 3\n",
    "\n",
    "\n",
    "# def generate_queries(question, existing_knowledge=None, query_num=1):\n",
    "#     input = [\n",
    "#         {\n",
    "#             \"type\": \"text\",\n",
    "#             \"content\": prompt_memory_retrieval.format(\n",
    "#                 question=question,\n",
    "#                 query_num=query_num,\n",
    "#                 existing_knowledge=existing_knowledge,\n",
    "#             ),\n",
    "#         }\n",
    "#     ]\n",
    "#     messages = generate_messages(input)\n",
    "#     model = \"gpt-4o-2024-11-20\"\n",
    "#     queries = None\n",
    "#     for i in range(MAX_RETRIES):\n",
    "#         print(f\"Generating queries {i} times\")\n",
    "#         queries = get_response_with_retry(model, messages)[0]\n",
    "#         queries = validate_and_fix_python_list(queries)\n",
    "#         if queries is not None:\n",
    "#             break\n",
    "#     if queries is None:\n",
    "#         raise Exception(\"Failed to generate queries\")\n",
    "#     return queries\n",
    "\n",
    "\n",
    "# def retrieve_from_videograph(videograph, question, topk=3):\n",
    "#     queries = generate_queries(question)\n",
    "#     print(f\"Queries: {queries}\")\n",
    "\n",
    "#     model = \"text-embedding-3-large\"\n",
    "#     query_embeddings = parallel_get_embedding(model, queries)[0]\n",
    "\n",
    "#     related_nodes = []\n",
    "\n",
    "#     for query_embedding in query_embeddings:\n",
    "#         nodes = videograph.search_text_nodes(query_embedding)\n",
    "#         related_nodes.extend(nodes)\n",
    "\n",
    "#     related_nodes = list(set(related_nodes))\n",
    "#     return related_nodes\n",
    "\n",
    "\n",
    "# question = \"Denny\"\n",
    "# retrieved_nodes = retrieve_from_videograph(video_graph, question)\n",
    "# print(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "9e153c75-6630-46b6-af1a-76d1a8277f1c",
  "filePath": "/mnt/bn/videonasi18n/longlin.kylin/tce-face-extraction/demo.ipynb",
  "kernelspec": {
   "display_name": "vlm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
