{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bytedtrace/__init__.py:108: UserWarning: [bytedtrace] global tracer is already initialized.\n",
      "  warnings.warn('[bytedtrace] global tracer is already initialized.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "from videograph import VideoGraph\n",
    "from utils.general import *\n",
    "from utils.video_processing import *\n",
    "from utils.chat_api import *\n",
    "from prompts import *\n",
    "\n",
    "from face_processing import process_faces\n",
    "from voice_processing import process_voices\n",
    "from memory_processing import (\n",
    "    process_captions,\n",
    "    generate_captions_and_thinkings_with_ids,\n",
    ")\n",
    "from retrieve import answer_with_retrieval\n",
    "\n",
    "processing_config = json.load(open(\"configs/processing_config.json\"))\n",
    "memory_config = json.load(open(\"configs/memory_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(video_graph, base64_video, base64_frames, base64_audio, clip_id):\n",
    "\n",
    "    id2voices = process_voices(video_graph, base64_audio, base64_video)\n",
    "    print(\"Finish processing voices\")\n",
    "\n",
    "    print(f\"processing {len(base64_frames)} frames...\")\n",
    "\n",
    "    id2faces = process_faces(video_graph, base64_frames)\n",
    "    # print(id2faces.keys())\n",
    "    print(\"Finish processing faces\")\n",
    "\n",
    "    episodic_captions, semantic_captions = generate_captions_and_thinkings_with_ids(\n",
    "        video_graph,\n",
    "        base64_video,\n",
    "        base64_frames,\n",
    "        base64_audio,\n",
    "        id2faces,\n",
    "        id2voices,\n",
    "    )\n",
    "\n",
    "    process_captions(video_graph, episodic_captions, clip_id, type=\"episodic\")\n",
    "    process_captions(video_graph, semantic_captions, clip_id, type=\"semantic\")\n",
    "\n",
    "    print(\"Finish processing segment\")\n",
    "\n",
    "\n",
    "def streaming_process_video(\n",
    "    video_graph, video_path, interval_seconds, fps, segment_limit=None\n",
    "):\n",
    "    \"\"\"Process video segments at specified intervals with given fps.\n",
    "\n",
    "    Args:\n",
    "        video_graph (VideoGraph): Graph object to store video information\n",
    "        video_path (str): Path to the video file or directory containing clips\n",
    "        interval_seconds (float): Time interval between segments in seconds\n",
    "        fps (float): Frames per second to extract from each segment\n",
    "\n",
    "    Returns:\n",
    "        None: Updates video_graph in place with processed segments\n",
    "    \"\"\"\n",
    "    if os.path.isfile(video_path):\n",
    "        # Process single video file\n",
    "        video_info = get_video_info(video_path)\n",
    "        print(video_info)\n",
    "\n",
    "        # Process each interval\n",
    "        count = 0\n",
    "        for start_time in tqdm(np.arange(0, video_info[\"duration\"], interval_seconds)):\n",
    "            if start_time + interval_seconds > video_info[\"duration\"]:\n",
    "                break\n",
    "\n",
    "            print(\"=\" * 20)\n",
    "\n",
    "            print(f\"Loading {count}-th clip starting at {start_time} seconds...\")\n",
    "            base64_video, base64_frames, base64_audio = process_video_clip(\n",
    "                video_path, start_time, interval_seconds, fps, audio_format=\"wav\"\n",
    "            )\n",
    "\n",
    "            # check dtype\n",
    "            # print(type(base64_video), type(base64_frames[0]), type(base64_audio))\n",
    "\n",
    "            # Process frames for this interval\n",
    "            if base64_frames:\n",
    "                print(\n",
    "                    f\"Starting processing {count}-th clip starting at {start_time} seconds...\"\n",
    "                )\n",
    "                process_segment(\n",
    "                    video_graph, base64_video, base64_frames, base64_audio, count\n",
    "                )\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if segment_limit is not None and count >= segment_limit:\n",
    "                break\n",
    "\n",
    "    elif os.path.isdir(video_path):\n",
    "        # Process directory of numbered clips\n",
    "        files = os.listdir(video_path)\n",
    "        # Filter for video files and sort by numeric value in filename\n",
    "        video_files = [\n",
    "            f for f in files if any(f.endswith(ext) for ext in [\".mp4\", \".avi\", \".mov\"])\n",
    "        ]\n",
    "        video_files.sort(key=lambda x: int(\"\".join(filter(str.isdigit, x))))\n",
    "\n",
    "        for count, video_file in enumerate(tqdm(video_files)):\n",
    "            if segment_limit is not None and count >= segment_limit:\n",
    "                break\n",
    "            print(\"=\" * 20)\n",
    "            full_path = os.path.join(video_path, video_file)\n",
    "            print(f\"Starting processing {count}-th clip: {full_path}\")\n",
    "\n",
    "            base64_video, base64_frames, base64_audio = process_video_clip(\n",
    "                full_path, 0, None, fps, audio_format=\"wav\"\n",
    "            )\n",
    "\n",
    "            if base64_frames:\n",
    "                process_segment(\n",
    "                    video_graph, base64_video, base64_frames, base64_audio, count\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processing_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# video paths can be paths to directories or paths to mp4 files\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m video_paths \u001b[39m=\u001b[39m processing_config[\u001b[39m\"\u001b[39m\u001b[39mvideo_paths\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m video_path \u001b[39min\u001b[39;00m video_paths:\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     video_graph \u001b[39m=\u001b[39m VideoGraph(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmemory_config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processing_config' is not defined"
     ]
    }
   ],
   "source": [
    "# video paths can be paths to directories or paths to mp4 files\n",
    "video_paths = processing_config[\"video_paths\"]\n",
    "\n",
    "for video_path in video_paths:\n",
    "\n",
    "    video_graph = VideoGraph(**memory_config)\n",
    "\n",
    "    streaming_process_video(\n",
    "        video_graph,\n",
    "        video_path,\n",
    "        processing_config[\"interval_seconds\"],\n",
    "        processing_config[\"fps\"],\n",
    "        processing_config[\"segment_limit\"],\n",
    "    )\n",
    "\n",
    "    video_graph.refresh_equivalences()\n",
    "\n",
    "    save_dir = \"data/video_graphs\"\n",
    "    save_video_graph(\n",
    "        video_graph, video_path, save_dir, (processing_config, memory_config)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video graph from data/video_graphs/5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75.pkl\n",
      "Generating queries 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 12:50:52,173 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: ['What actions are performed by <face_0> in the video?', 'What actions are performed by <face_1> in the video?', 'What actions are performed by <face_4> in the video?', 'What interactions occur between <face_4> and <face_9>?', 'What tasks or activities are assigned by <voice_0> to the individuals?', 'What is the purpose of the meeting or gathering in the video?', 'What are the physical gestures or movements of <face_9>?', 'What are the roles or responsibilities of the individuals introduced by <voice_0>?', \"What is the significance of <face_1>'s role as an executive or presenter?\", \"What are the reactions or responses of the participants to <voice_0>'s introduction?\"]\n",
      "New memories from clip 3: ['In a brightly lit studio setting, <face_10>, <face_8>, <face_9>, <face_11>, and <face_12> stand facing four individuals seated at a table draped in black cloth: <face_7>, <character_0>, <face_6>, and <face_5>.', \"<face_10> wears a black jacket and light blue jeans. <face_9> is dressed in a crisp white, long-sleeved shirt and khaki pants. <character_3>, positioned between them, wears a vibrant purple blazer and black pants, her sequined crop top adding a touch of sparkle. <face_11>, standing next to <character_3>, sports a dark blue jacket and gray pants. <face_12>, at the end of the line, is clad in a camouflage jacket and brown pants. On the table in front of the seated group, an 'ON AIR' sign glows, and water bottles and paper coffee cups are neatly arranged.\", '<face_7> wears a black hoodie, while <character_0>, next to him, sports a black and white baseball jersey.  <face_6>, in a dark blue denim jacket, sits next to <character_0>. <face_5>, in a beige t-shirt, sits at the end of the table.', '<face_10> introduces himself as Damar R., a fashion designer. <character_3> introduces herself as Tori K. and says her family works in the oil business. This elicits excited reactions from the seated group. <face_4> reiterates what <character_3> said about her family’s involvement in the oil business.', '<character_0> asks why white people share their wealth.  Everyone in the room reacts with laughter.', '<voice_54> asks <character_4> to introduce himself. <character_4> says his name is Labone James.', \"The seated group reacts with amusement to <face_9>'s name, with <character_2> commenting that it's 'broke' and needs 'a Labone'.\", 'Equivalence: <face_8>, <character_3>', 'Equivalence: <face_9>, <character_4>', 'Equivalence: <face_4>, <character_0>', 'Equivalence: <face_7>, <character_2>', '<character_1> is a fashion designer.', \"<character_3>'s family is in the oil business.\", \"<character_4>'s name, Labone, is considered unusual and amusing by the seated group.\", 'The seated group, including <face_4>, <face_5>, <face_6>, and <character_2>, appear to be reacting to and commenting on the standing group.', 'There seems to be a playful dynamic between the two groups, possibly part of a game or interview setting.']\n",
      "New memories from clip 4: ['In a brightly lit studio, five individuals, <face_9>, <face_8>, <face_11>, <face_12>, and <face_10>, stand before a panel of four, <face_7>, <character_0>, <face_6>, and <face_5>, who are seated at a table covered with a black tablecloth.', '<face_10> is dressed in a black jacket and light blue jeans. <face_9> wears a white, long-sleeved shirt and khaki pants. <face_8>, positioned between <face_9> and <character_5>, wears a purple blazer over a sequined crop top and black pants. <character_5> wears a dark blue jacket and gray pants. <face_12> is dressed in a camouflage jacket and brown pants.', \"At the table, <face_7> is dressed in a black hoodie, while <character_0>, seated next to him, wears a black and white baseball jersey. <face_6>, in a dark blue denim jacket, sits next to <character_0>. <face_5> wears a beige t-shirt. An 'ON AIR' sign is placed on the table, along with water bottles, coffee cups, and markers.\", \"<voice_54> asks <character_4> what he does for a living. <character_4> replies that he is a creative. The panel at the table reacts with laughter, with <face_4> putting his head down on the table. <voice_72> and <voice_73> remark that <character_4> 'might be broke'. <voice_54> then asks <character_4> for his name, to which he clarifies, stating his OnlyFans handle is 'Labone James'.\", \"The seated group expresses surprise. <voice_54> then asks <character_5> his name and occupation. <character_5> introduces himself as Stuart Thompson, a professor. The panel reacts with surprise, with <voice_54> commenting that professors don't usually make millions.\", 'Equivalence: <face_11>, <character_5>', '<character_4> is likely involved in adult content creation given his OnlyFans handle.', '<face_5>, <face_4>, <face_6>, and <character_2> appear to be judging or evaluating the other individuals based on their professions.', '<character_0> seems particularly surprised and amused by the revealed occupations.', \"The panel's reaction to <character_3>'s family's oil business suggests they associate it with wealth.\"]\n",
      "New memories from clip 1: [\"In a brightly lit studio, four individuals, <face_7>, <character_0>, <face_6>, and <face_5>, are seated at a black table draped with a black tablecloth. An 'ON AIR' sign is placed on the table. <face_7> wears a black hoodie, <character_0> a black and white baseball jersey, <face_6> a dark blue denim jacket, and <face_5> a light beige t-shirt. They have water bottles and paper cups in front of them.\", 'Five other individuals stand behind the seated group. <face_10> wears a black jacket and light blue jeans. <face_9> wears a white long-sleeved shirt and khaki pants. <face_2> wears a purple blazer and black pants, revealing a sparkly crop top. <character_5> wears a dark blue jacket and gray pants. <face_12> wears a camouflage jacket and brown pants.', \"<character_2> comments on <face_12>'s penny loafers, saying, 'And he got penny loafers on with no penny in it. So it's like y'all don't even know how much I got.'\", \"<character_1> remarks, 'But then again, millionaires don't dress up.'\", \"<character_0> adds, 'They rarely wear like four, three and four very like calm 'cause they don't make, you know, they don't express themselves.'\", \"<voice_3> then analyzes <character_5>, saying, 'He looks a little bit too well put together...You look like you're pretending that you have money, but you work at Gelsons.'\", \"<character_0> shifts the focus to <face_2>, stating, 'We're gazing over number two, our beautiful lady here.'\", 'Equivalence: <face_7>, <character_2>', 'Equivalence: <face_10>, <character_1>', 'Equivalence: <face_12>, <character_0>', '<character_5> is perceived as trying too hard to appear wealthy.', '<face_12> is being assessed by the group at the table based on his appearance.', \"The individuals at the table are analyzing and commenting on the standing individuals' clothing and overall presentation.\", 'The group at the table is likely participating in a game or challenge where they judge others based on first impressions.']\n",
      "New memories from clip 0: [\"In a brightly lit studio setting, four individuals, <face_7>, <character_0>, <face_6>, and <face_5>, are seated at a black table covered with a black tablecloth. <face_7> is identified as Denny, <character_0> as Herm, <face_6> as Aaron, and <face_5> as JC.\\xa0 They are joined by five other individuals standing behind them. An 'ON AIR' sign sits on the table.\", '<character_0> introduces the four seated individuals by name. <voice_1> states that only one of the five standing individuals is a millionaire and the other four are lying.', '<face_10>, wearing a black jacket and jeans, makes a gesture with his hands and is identified by <character_1> as broke. The group laughs.', \"<character_1>, one of the seated individuals, says that he had to pay <face_10> because he was a guest of 'Black Privilege.' <character_1> also comments on <face_12>'s 'elite' shoes.\", \"<character_2> observes that one of the standing individuals is not wearing socks and states that it means he's comfortable.\", 'Equivalence: <face_7>, <character_1>', 'Equivalence: <face_4>, <character_0>', 'Equivalence: <face_6>, <character_2>', '<character_1> is perceived as broke by the seated group.', \"<character_1> was a guest on 'Black Privilege'.\", \"<face_12>'s shoes are considered 'elite' by <character_1>.\", '<face_11> is not wearing socks and is considered comfortable with his wealth by <character_2>.']\n",
      "New memories from clip 2: [\"In a brightly lit studio setting, four individuals, <face_7>, <character_0>, <face_6>, and <face_5>, are seated at a table covered with a black tablecloth. <face_7> wears a black hoodie. <character_0> wears a black and white baseball jersey. <face_6> is dressed in a dark blue denim jacket. <face_5> wears a light beige t-shirt.  An 'ON AIR' sign sits on the table in front of them.  Bottled water and paper coffee cups are arranged on the table for each person.\", 'Behind the seated group, another group of five stand. <face_10> wears a black jacket and light blue jeans. <face_9> wears a white long-sleeved shirt and khaki pants. <face_2> wears a purple blazer and black pants, her sparkly crop top visible. <character_5> is dressed in a dark blue jacket and gray pants. <face_12> wears a camouflage jacket and brown pants.', \"<voice_39> asks if they are gazing. <voice_40>, <face_4>, clarifies that he meant to ask what the word is for not paying attention. <character_1> clarifies further by saying 'overlooking.'\", \"<character_2> compliments <face_2>'s appearance and attire. <character_2> comments that <face_2> looks like she has 'new money.'\", '<voice_0> suggests they dive into the individuals one by one and asks <face_10> to state their name and job. <face_10> introduces himself as Demar Randy, <character_1>.', 'Equivalence: <face_10>, <character_1>', '<character_0> appears to be facilitating the discussion, prompting others for their names and jobs.', '<face_3> observes <face_2> and makes a comment about their perceived financial status based on their appearance.', 'The group at the table (<face_4>, <face_7>, <face_6>, and <face_5>) seems to be engaged in some form of interview or assessment of the standing group (<face_10>, <face_9>, <face_2>, <character_5>, and <face_12>).', '<face_3> displays a keen interest in fashion and social status, making quick judgments about others based on their clothing and accessories.']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'\\n    \"clip_1\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# video_graph.refresh_equivalences()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# question = 'What does Demar Randy wear?'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# question = \"Who has an OnlyFans account?\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat are the people doing in the video?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m answer \u001b[39m=\u001b[39m answer_with_retrieval(\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     video_graph,\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     question,\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     query_num\u001b[39m=\u001b[39;49mprocessing_config[\u001b[39m\"\u001b[39;49m\u001b[39mquery_num\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     topk\u001b[39m=\u001b[39;49mprocessing_config[\u001b[39m\"\u001b[39;49m\u001b[39mtopk\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# video_graph.summarize(logging=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# save_dir = \"data/video_graphs\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# save_video_graph(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#     video_graph, None, save_dir, None, file_name='5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75_augmented.pkl'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# video_graph.visualize()\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/retrieve.py:186\u001b[0m, in \u001b[0;36manswer_with_retrieval\u001b[0;34m(video_graph, question, query_num, topk, auto_refresh)\u001b[0m\n\u001b[1;32m    180\u001b[0m related_memories \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39msorted\u001b[39m(related_memories\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39m]))\n\u001b[1;32m    182\u001b[0m \u001b[39m# replace the entities in the memories with the character mappings\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m [\n\u001b[1;32m    184\u001b[0m     {\n\u001b[1;32m    185\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 186\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt_answer_with_retrieval_clipwise\u001b[39m.\u001b[39;49mformat(\n\u001b[1;32m    187\u001b[0m             question\u001b[39m=\u001b[39;49mquestion,\n\u001b[1;32m    188\u001b[0m             related_memories\u001b[39m=\u001b[39;49mjson\u001b[39m.\u001b[39;49mdumps({\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclip_\u001b[39;49m\u001b[39m{\u001b[39;49;00mk\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m related_memories\u001b[39m.\u001b[39;49mitems()}),\n\u001b[1;32m    189\u001b[0m         ),\n\u001b[1;32m    190\u001b[0m     }\n\u001b[1;32m    191\u001b[0m ]\n\u001b[1;32m    192\u001b[0m messages \u001b[39m=\u001b[39m generate_messages(\u001b[39minput\u001b[39m)\n\u001b[1;32m    193\u001b[0m model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpt-4o-2024-11-20\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: '\\n    \"clip_1\"'"
     ]
    }
   ],
   "source": [
    "video_graph_path = \"data/video_graphs/5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75.pkl\"\n",
    "video_graph = load_video_graph(video_graph_path)\n",
    "# for text_node in video_graph.text_nodes:\n",
    "#     print(video_graph.nodes[text_node].metadata['contents'])\n",
    "# for nodes, weight in video_graph.edges.items():\n",
    "#     if weight > 1:\n",
    "#         if video_graph.nodes[nodes[0]].type in [\"episodic\", \"semantic\"]:\n",
    "#            print(video_graph.nodes[nodes[0]].metadata['contents'])\n",
    "#         else:\n",
    "#            print(video_graph.nodes[nodes[1]].metadata['contents'])\n",
    "#         print(weight)\n",
    "\n",
    "video_graph.text_matching_threshold = processing_config[\"retrieval_threshold\"]\n",
    "# video_graph.refresh_equivalences()\n",
    "\n",
    "# question = 'What does Demar Randy wear?'\n",
    "# question = \"Who has an OnlyFans account?\"\n",
    "question = \"What are the people doing in the video?\"\n",
    "answer = answer_with_retrieval(\n",
    "    video_graph,\n",
    "    question,\n",
    "    query_num=processing_config[\"query_num\"],\n",
    "    topk=processing_config[\"topk\"],\n",
    ")\n",
    "\n",
    "# video_graph.summarize(logging=True)\n",
    "# save_dir = \"data/video_graphs\"\n",
    "# save_video_graph(\n",
    "#     video_graph, None, save_dir, None, file_name='5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75_augmented.pkl'\n",
    "# )\n",
    "# video_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chat_api import *\n",
    "from utils.general import plot_cosine_similarity_distribution\n",
    "\n",
    "video_graph_path = \"data/video_graphs/5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75.pkl\"\n",
    "video_graph = load_video_graph(video_graph_path)\n",
    "\n",
    "graph_embeddings = []\n",
    "\n",
    "for id, node in video_graph.nodes.items():\n",
    "    if node.type in [\"episodic\", \"semantic\"]:\n",
    "        graph_embeddings.extend(node.embeddings)\n",
    "\n",
    "# texts = [\"Clothing style of Demar Randy\", \"<voice_44> introduces himself as Demar Randy.\"]\n",
    "texts = [\"<face_4> points at <face_9>.\"]\n",
    "embs = parallel_get_embedding(\"text-embedding-3-large\", texts)[0]\n",
    "\n",
    "plot_cosine_similarity_distribution(graph_embeddings, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_node in video_graph.text_nodes:\n",
    "    print(video_graph.nodes[text_node].metadata[\"contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from retrieve import retrieve_from_videograph\n",
    "# from videograph import VideoGraph\n",
    "# from utils.chat_api import (\n",
    "#     generate_messages,\n",
    "#     get_response_with_retry,\n",
    "#     parallel_get_embedding,\n",
    "# )\n",
    "# from utils.general import validate_and_fix_python_list\n",
    "# from prompts import prompt_memory_retrieval\n",
    "\n",
    "# MAX_RETRIES = 3\n",
    "\n",
    "\n",
    "# def generate_queries(question, existing_knowledge=None, query_num=1):\n",
    "#     input = [\n",
    "#         {\n",
    "#             \"type\": \"text\",\n",
    "#             \"content\": prompt_memory_retrieval.format(\n",
    "#                 question=question,\n",
    "#                 query_num=query_num,\n",
    "#                 existing_knowledge=existing_knowledge,\n",
    "#             ),\n",
    "#         }\n",
    "#     ]\n",
    "#     messages = generate_messages(input)\n",
    "#     model = \"gpt-4o-2024-11-20\"\n",
    "#     queries = None\n",
    "#     for i in range(MAX_RETRIES):\n",
    "#         print(f\"Generating queries {i} times\")\n",
    "#         queries = get_response_with_retry(model, messages)[0]\n",
    "#         queries = validate_and_fix_python_list(queries)\n",
    "#         if queries is not None:\n",
    "#             break\n",
    "#     if queries is None:\n",
    "#         raise Exception(\"Failed to generate queries\")\n",
    "#     return queries\n",
    "\n",
    "\n",
    "# def retrieve_from_videograph(videograph, question, topk=3):\n",
    "#     queries = generate_queries(question)\n",
    "#     print(f\"Queries: {queries}\")\n",
    "\n",
    "#     model = \"text-embedding-3-large\"\n",
    "#     query_embeddings = parallel_get_embedding(model, queries)[0]\n",
    "\n",
    "#     related_nodes = []\n",
    "\n",
    "#     for query_embedding in query_embeddings:\n",
    "#         nodes = videograph.search_text_nodes(query_embedding)\n",
    "#         related_nodes.extend(nodes)\n",
    "\n",
    "#     related_nodes = list(set(related_nodes))\n",
    "#     return related_nodes\n",
    "\n",
    "\n",
    "# question = \"Denny\"\n",
    "# retrieved_nodes = retrieve_from_videograph(video_graph, question)\n",
    "# print(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "9e153c75-6630-46b6-af1a-76d1a8277f1c",
  "filePath": "/mnt/bn/videonasi18n/longlin.kylin/tce-face-extraction/demo.ipynb",
  "kernelspec": {
   "display_name": "vlm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
