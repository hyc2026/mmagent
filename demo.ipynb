{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bytedtrace/__init__.py:108: UserWarning: [bytedtrace] global tracer is already initialized.\n",
      "  warnings.warn('[bytedtrace] global tracer is already initialized.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "from videograph import VideoGraph\n",
    "from utils.general import *\n",
    "from utils.video_processing import *\n",
    "from utils.chat_api import *\n",
    "from prompts import *\n",
    "\n",
    "from face_processing import process_faces\n",
    "from voice_processing import process_voices\n",
    "from memory_processing import (\n",
    "    process_captions,\n",
    "    generate_captions_and_thinkings_with_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(video_graph, base64_video, base64_frames, base64_audio):\n",
    "\n",
    "    id2voices = process_voices(video_graph, base64_audio, base64_video)\n",
    "    print(\"Finish processing voices\")\n",
    "\n",
    "    print(f\"processing {len(base64_frames)} frames...\")\n",
    "\n",
    "    id2faces = process_faces(video_graph, base64_frames)\n",
    "    # print(id2faces.keys())\n",
    "    print(\"Finish processing faces\")\n",
    "\n",
    "    episodic_captions, semantic_captions = generate_captions_and_thinkings_with_ids(\n",
    "        video_graph,\n",
    "        base64_video,\n",
    "        base64_frames,\n",
    "        base64_audio,\n",
    "        id2faces,\n",
    "        id2voices,\n",
    "    )\n",
    "\n",
    "    process_captions(video_graph, episodic_captions, type=\"episodic\")\n",
    "    process_captions(video_graph, semantic_captions, type=\"semantic\")\n",
    "\n",
    "    print(\"Finish processing segment\")\n",
    "\n",
    "\n",
    "def streaming_process_video(\n",
    "    video_graph, video_path, interval_seconds, fps, segment_limit=None\n",
    "):\n",
    "    \"\"\"Process video segments at specified intervals with given fps.\n",
    "\n",
    "    Args:\n",
    "        video_graph (VideoGraph): Graph object to store video information\n",
    "        video_path (str): Path to the video file or directory containing clips\n",
    "        interval_seconds (float): Time interval between segments in seconds\n",
    "        fps (float): Frames per second to extract from each segment\n",
    "\n",
    "    Returns:\n",
    "        None: Updates video_graph in place with processed segments\n",
    "    \"\"\"\n",
    "    if os.path.isfile(video_path):\n",
    "        # Process single video file\n",
    "        video_info = get_video_info(video_path)\n",
    "        print(video_info)\n",
    "\n",
    "        # Process each interval\n",
    "        count = 0\n",
    "        for start_time in np.arange(0, video_info[\"duration\"], interval_seconds):\n",
    "            if start_time + interval_seconds > video_info[\"duration\"]:\n",
    "                break\n",
    "\n",
    "            print(\"=\" * 20)\n",
    "            count += 1\n",
    "\n",
    "            print(f\"Loading {count}-th clip starting at {start_time} seconds...\")\n",
    "            base64_video, base64_frames, base64_audio = process_video_clip(\n",
    "                video_path, start_time, interval_seconds, fps, audio_format=\"wav\"\n",
    "            )\n",
    "\n",
    "            # check dtype\n",
    "            # print(type(base64_video), type(base64_frames[0]), type(base64_audio))\n",
    "\n",
    "            # Process frames for this interval\n",
    "            if base64_frames:\n",
    "                print(\n",
    "                    f\"Starting processing {count}-th clip starting at {start_time} seconds...\"\n",
    "                )\n",
    "                process_segment(\n",
    "                    video_graph,\n",
    "                    base64_video,\n",
    "                    base64_frames,\n",
    "                    base64_audio,\n",
    "                )\n",
    "\n",
    "            if segment_limit is not None and count >= segment_limit:\n",
    "                break\n",
    "\n",
    "    elif os.path.isdir(video_path):\n",
    "        # Process directory of numbered clips\n",
    "        files = os.listdir(video_path)\n",
    "        # Filter for video files and sort by numeric value in filename\n",
    "        video_files = [\n",
    "            f for f in files if any(f.endswith(ext) for ext in [\".mp4\", \".avi\", \".mov\"])\n",
    "        ]\n",
    "        video_files.sort(key=lambda x: int(\"\".join(filter(str.isdigit, x))))\n",
    "\n",
    "        for count, video_file in enumerate(video_files, 1):\n",
    "            print(\"=\" * 20)\n",
    "            full_path = os.path.join(video_path, video_file)\n",
    "            print(f\"Processing clip {count}: {full_path}\")\n",
    "\n",
    "            base64_video, base64_frames, base64_audio = process_video_clip(\n",
    "                full_path, 0, None, fps, audio_format=\"wav\"\n",
    "            )\n",
    "\n",
    "            if base64_frames:\n",
    "                process_segment(\n",
    "                    video_graph,\n",
    "                    base64_video,\n",
    "                    base64_frames,\n",
    "                    base64_audio,\n",
    "                )\n",
    "\n",
    "            if segment_limit is not None and count >= segment_limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_config = json.load(open(\"configs/processing_config.json\"))\n",
    "memory_config = json.load(open(\"configs/memory_config.json\"))\n",
    "# video paths can be paths to directories or paths to mp4 files\n",
    "video_paths = processing_config[\"video_paths\"]\n",
    "\n",
    "for video_path in video_paths:\n",
    "\n",
    "    video_graph = VideoGraph(**memory_config)\n",
    "\n",
    "    streaming_process_video(\n",
    "        video_graph,\n",
    "        video_path,\n",
    "        processing_config[\"interval_seconds\"],\n",
    "        processing_config[\"fps\"],\n",
    "        processing_config[\"segment_limit\"],\n",
    "    )\n",
    "\n",
    "    save_dir = \"data/video_graphs\"\n",
    "    save_video_graph(\n",
    "        video_graph, video_path, save_dir, (processing_config, memory_config)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video graph from data/video_graphs/5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75.pkl\n",
      "================================================================================\n",
      "Cluster 0 has 1 nodes: [36]\n",
      "--------------------------------------------------------------------------------\n",
      "Node 36 [edge weight]: 1.0\n",
      "Node 36 [content]: Equivalence: <face_3>, <voice_0>\n",
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 03:57:49,590 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster 0 has 1 nodes: [18]\n",
      "--------------------------------------------------------------------------------\n",
      "Node 18 [edge weight]: 1.0\n",
      "Node 18 [content]: Equivalence: <face_9>, <voice_1>\n",
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 03:57:50,065 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster 0 has 2 nodes: [17, 35]\n",
      "--------------------------------------------------------------------------------\n",
      "Node 17 [edge weight]: 1.0\n",
      "Node 17 [content]: Equivalence: <face_3>, <voice_2>\n",
      "--------------------------------------------------------------------------------\n",
      "Node 35 [edge weight]: 1.0\n",
      "Node 35 [content]: Equivalence: <face_6>, <voice_2>\n",
      "********************************************************************************\n",
      "Cluster 0 has 1 nodes after filtering: [17]\n",
      "--------------------------------------------------------------------------------\n",
      "Node 17 [edge weight]: 1.0\n",
      "Node 17 [content]: Equivalence: <face_3>, <voice_2>\n",
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 03:57:50,652 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster 0 has 1 nodes: [59]\n",
      "--------------------------------------------------------------------------------\n",
      "Node 59 [edge weight]: 1.0\n",
      "Node 59 [content]: Equivalence: <face_9>, <voice_44>.\n",
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 03:57:51,532 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster 0 has 1 nodes: [75]\n",
      "--------------------------------------------------------------------------------\n",
      "Node 75 [edge weight]: 1.0\n",
      "Node 75 [content]: Equivalence: <face_7>, <voice_64>\n",
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 03:57:52,239 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster 0 has 1 nodes: [92]\n",
      "--------------------------------------------------------------------------------\n",
      "Node 92 [edge weight]: 1.0\n",
      "Node 92 [content]: Equivalence: <face_22>, <voice_66>\n",
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 03:57:52,759 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster 0 has 1 nodes: [76]\n",
      "--------------------------------------------------------------------------------\n",
      "Node 76 [edge weight]: 3.0\n",
      "Node 76 [content]: Equivalence: <face_8>, <voice_67>\n",
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 03:57:53,270 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Cluster 0 has 1 nodes: [91]\n",
      "--------------------------------------------------------------------------------\n",
      "Node 91 [edge weight]: 1.0\n",
      "Node 91 [content]: Equivalence: <face_10>, <voice_82>\n",
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 03:57:56,503 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'character_0': [3, 0, 2], 'character_1': [9, 1, 44], 'character_2': [7, 64], 'character_3': [22, 66], 'character_4': [8, 67], 'character_5': [10, 82]}\n"
     ]
    }
   ],
   "source": [
    "video_graph_path = \"data/video_graphs/5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75.pkl\"\n",
    "video_graph = load_video_graph(video_graph_path)\n",
    "# for text_node in video_graph.text_nodes:\n",
    "#     print(video_graph.nodes[text_node].metadata['contents'])\n",
    "# for nodes, weight in video_graph.edges.items():\n",
    "#     if weight > 1:\n",
    "#         if video_graph.nodes[nodes[0]].type in [\"episodic\", \"semantic\"]:\n",
    "#            print(video_graph.nodes[nodes[0]].metadata['contents'])\n",
    "#         else:\n",
    "#            print(video_graph.nodes[nodes[1]].metadata['contents'])\n",
    "#         print(weight)\n",
    "\n",
    "equivalences = video_graph.extract_equivalences()\n",
    "print(equivalences)\n",
    "           \n",
    "# video_graph.summarize(logging=True)\n",
    "# save_dir = \"data/video_graphs\"\n",
    "# save_video_graph(\n",
    "#     video_graph, None, save_dir, None, file_name='5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75_augmented.pkl'\n",
    "# )\n",
    "# video_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<voice_0> introduces four individuals named Denny, Herm, Aaron, and JC, who are seated at a table.']\n",
      "['<voice_0> mentions five other individuals who claim to be millionaires, but only one of them is telling the truth.']\n",
      "['<voice_1> suggests starting the evaluation based on appearances.']\n",
      "['<face_9>, wearing a black jacket and jeans, is identified by <voice_2> as not being a millionaire.']\n",
      "['<voice_2> states that <face_4>, because they were a guest of another person, had to be paid.']\n",
      "[\"<voice_2> comments that <face_8>'s shoes look expensive.\"]\n",
      "['Equivalence: <face_3>, <voice_2>']\n",
      "['Equivalence: <face_9>, <voice_1>']\n",
      "['<face_9> is perceived as not wealthy due to their attire.']\n",
      "['<face_4> may have received payment for their participation.']\n",
      "['<face_8> is perceived as wealthy due to their expensive footwear.']\n",
      "['<face_6> sits at the table, wearing a black hoodie.']\n",
      "['<face_22> sits at the table, wearing a blue Drexel University hoodie and glasses.']\n",
      "['<face_3> sits at the table, wearing a black and white baseball-style shirt.']\n",
      "['<face_5> sits at the table, wearing a blue jacket and a white beanie.']\n",
      "['<face_4> sits at the table, wearing a light-colored t-shirt with tattoos visible on his arms.']\n",
      "['<face_9> stands to the side, wearing a black jacket and light-colored jeans.']\n",
      "['<face_8> stands to the side, wearing a white long-sleeved shirt and sunglasses.']\n",
      "['<face_10> stands to the side, wearing a blue jacket and grey pants.']\n",
      "[\"<face_6> speaks, commenting on someone's penny loafers and suggesting they have a lot of money.\"]\n",
      "[\"<face_3> speaks, suggesting that millionaires don't dress up.\"]\n",
      "['<face_22> speaks and gestures with his hands while talking about how much money he has.']\n",
      "['<face_6> points and speaks, referring to another person in the scene.']\n",
      "['Equivalence: <face_6>, <voice_2>']\n",
      "['Equivalence: <face_3>, <voice_0>']\n",
      "['<face_6> and <face_22> appear to be commentators or judges, observing and providing feedback on others.']\n",
      "['<face_3> seems to be engaging in a playful debate with <face_6> about wealth and appearances.']\n",
      "['The group at the table (<face_6>, <face_22>, <face_3>, <face_5>, <face_4>) might be part of a panel or show judging the group standing (<face_9>, <face_8>, <face_10>).']\n",
      "['The video appears to be a segment from a show or competition, potentially about fashion or social status.']\n",
      "['<face_3> wears a black and white baseball-style shirt.']\n",
      "['<face_5> wears a blue jacket and a white beanie.']\n",
      "['<face_4> has tattoos visible on his arms.']\n",
      "['<face_9> wears a black jacket and light-colored jeans.']\n",
      "['<face_8> wears a white long-sleeved shirt and sunglasses.']\n",
      "['<face_10> wears a blue jacket and grey pants.']\n",
      "[\"<voice_0> comments on someone's appearance and suggests they may have a lot of money.\"]\n",
      "[\"<voice_2> suggests that millionaires don't dress up.\"]\n",
      "['<voice_41> asks <face_3> if they are gazing.']\n",
      "['<voice_42> asks what the English word is for not paying attention.']\n",
      "['<voice_43> explains that overlooking means not paying attention.']\n",
      "['<voice_44> introduces himself as Demar Randy.']\n",
      "['<face_22> gestures with his hands while talking about how much money he has.']\n",
      "['<face_6> points at another person during a conversation.']\n",
      "['Equivalence: <face_9>, <voice_44>.']\n",
      "['<face_3> and <face_5> appear to be close, engaging in a playful hand-clapping interaction.']\n",
      "[\"<face_6> seems observant and makes comments about other people's appearance and perceived financial status.\"]\n",
      "['<face_3> uses gestures while speaking.']\n",
      "['The group appears to be participating in a casual interview or discussion setting.']\n",
      "['<face_9> wears a black jacket and plaid tie.']\n",
      "['<face_7> wears a purple blazer and a sequined top.']\n",
      "['<face_8> wears a white long-sleeved shirt and sunglasses.']\n",
      "['<face_7> states that her family is in the oil business.']\n",
      "[\"<face_3> moves the 'ON AIR' sign on the table.\"]\n",
      "[\"<face_6>, <face_22>, <face_3>, <face_5>, and <face_4> react with surprise and excitement to <face_7>'s statement.\"]\n",
      "['<face_8> introduces himself as Labone James.']\n",
      "['Equivalence: <face_7>, <voice_64>']\n",
      "['Equivalence: <face_8>, <voice_67>']\n",
      "[\"<face_7> is likely wealthy due to her family's involvement in the oil business.\"]\n",
      "[\"<face_3>, <face_5>, and <face_4> appear to be friends or colleagues, reacting similarly to <face_7>'s statement.\"]\n",
      "['<face_6> and <face_22> seem more reserved in their reactions.']\n",
      "['<face_9> is associated with <face_7> and <face_8> and appears to be involved in the same situation, possibly a job interview or game show.']\n",
      "[\"<face_8>'s name, Labone, is unusual.\"]\n",
      "['<face_4> laughs and covers his mouth with his hand.']\n",
      "['<face_22> asks <face_8> what he does for a living.']\n",
      "['<face_8> responds that he is a creative.']\n",
      "['<face_6>, <face_22>, <face_3>, <face_5>, and <face_4> laugh.']\n",
      "['<face_8> reveals that he manages OnlyFans handles.']\n",
      "['<face_22> asks <face_10> his name and occupation.']\n",
      "['<face_10> says his name is Stuart Thompson and that he is a professor.']\n",
      "['<face_6> questions if professors make a lot of money.']\n",
      "['Equivalence: <face_10>, <voice_82>']\n",
      "['Equivalence: <face_22>, <voice_66>']\n",
      "['<face_6>, <face_22>, <face_3>, <face_5>, and <face_4> are likely colleagues or friends, judging by their shared laughter and informal conversation.']\n",
      "['<face_8> initially misleads the others about his profession, perhaps due to embarrassment or a desire to appear more successful.']\n",
      "[\"<face_10>'s profession as a professor contrasts with <face_8>'s OnlyFans handle management, leading to surprise and amusement among the group.\"]\n",
      "[\"<face_6>'s question about professors' salaries suggests a curiosity about income disparities between different professions.\"]\n"
     ]
    }
   ],
   "source": [
    "for text_node in video_graph.text_nodes:\n",
    "    print(video_graph.nodes[text_node].metadata['contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from retrieve import retrieve_from_videograph\n",
    "# from videograph import VideoGraph\n",
    "# from utils.chat_api import (\n",
    "#     generate_messages,\n",
    "#     get_response_with_retry,\n",
    "#     parallel_get_embedding,\n",
    "# )\n",
    "# from utils.general import validate_and_fix_python_list\n",
    "# from prompts import prompt_memory_retrieval\n",
    "\n",
    "# MAX_RETRIES = 3\n",
    "\n",
    "\n",
    "# def generate_queries(question, existing_knowledge=None, query_num=1):\n",
    "#     input = [\n",
    "#         {\n",
    "#             \"type\": \"text\",\n",
    "#             \"content\": prompt_memory_retrieval.format(\n",
    "#                 question=question,\n",
    "#                 query_num=query_num,\n",
    "#                 existing_knowledge=existing_knowledge,\n",
    "#             ),\n",
    "#         }\n",
    "#     ]\n",
    "#     messages = generate_messages(input)\n",
    "#     model = \"gpt-4o-2024-11-20\"\n",
    "#     queries = None\n",
    "#     for i in range(MAX_RETRIES):\n",
    "#         print(f\"Generating queries {i} times\")\n",
    "#         queries = get_response_with_retry(model, messages)[0]\n",
    "#         queries = validate_and_fix_python_list(queries)\n",
    "#         if queries is not None:\n",
    "#             break\n",
    "#     if queries is None:\n",
    "#         raise Exception(\"Failed to generate queries\")\n",
    "#     return queries\n",
    "\n",
    "\n",
    "# def retrieve_from_videograph(videograph, question, topk=3):\n",
    "#     queries = generate_queries(question)\n",
    "#     print(f\"Queries: {queries}\")\n",
    "\n",
    "#     model = \"text-embedding-3-large\"\n",
    "#     query_embeddings = parallel_get_embedding(model, queries)[0]\n",
    "\n",
    "#     related_nodes = []\n",
    "\n",
    "#     for query_embedding in query_embeddings:\n",
    "#         nodes = videograph.search_text_nodes(query_embedding)\n",
    "#         related_nodes.extend(nodes)\n",
    "\n",
    "#     related_nodes = list(set(related_nodes))\n",
    "#     return related_nodes\n",
    "\n",
    "\n",
    "# question = \"Denny\"\n",
    "# retrieved_nodes = retrieve_from_videograph(video_graph, question)\n",
    "# print(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "9e153c75-6630-46b6-af1a-76d1a8277f1c",
  "filePath": "/mnt/bn/videonasi18n/longlin.kylin/tce-face-extraction/demo.ipynb",
  "kernelspec": {
   "display_name": "vlm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
