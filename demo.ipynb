{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\n",
      "  warnings.warn(\"bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\")\n",
      "/usr/local/lib/python3.9/dist-packages/bytedtrace/__init__.py:108: UserWarning: [bytedtrace] global tracer is already initialized.\n",
      "  warnings.warn('[bytedtrace] global tracer is already initialized.')\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from videograph import VideoGraph\n",
    "from utils.general import *\n",
    "from utils.video_processing import *\n",
    "from utils.chat_api import *\n",
    "from prompts import *\n",
    "\n",
    "from retrieve import answer_with_retrieval\n",
    "from generate_memory import streaming_process_video\n",
    "\n",
    "processing_config = json.load(open(\"configs/processing_config.json\"))\n",
    "memory_config = json.load(open(\"configs/memory_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video paths can be paths to directories or paths to mp4 files\n",
    "video_paths = [\"/mnt/hdfs/foundation/longlin.kylin/mmagent/data/test_video_clips/test2\"]\n",
    "save_dir = processing_config[\"save_dir\"]\n",
    "max_workers = 1\n",
    "\n",
    "\n",
    "def process_single_video(video_path):\n",
    "    video_graph = VideoGraph(**memory_config)\n",
    "    streaming_process_video(video_graph, video_path)\n",
    "\n",
    "\n",
    "# Process videos in parallel using ThreadPoolExecutor with max_workers limit\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Process videos in parallel using map\n",
    "    list(\n",
    "        tqdm(\n",
    "            executor.map(process_single_video, video_paths),\n",
    "            total=len(video_paths),\n",
    "            desc=\"Processing videos\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_graph_path = (\n",
    "    \"data/mems/5-Poor-People-vs-1-Secret-Millionaire_60_5_-1_10_20_0.3_0.6.pkl\"\n",
    ")\n",
    "video_graph = load_video_graph(video_graph_path)\n",
    "\n",
    "question = \"How might the social development of children living in military families be affected?\"\n",
    "\n",
    "answer = answer_with_retrieval(\n",
    "    video_graph,\n",
    "    question,\n",
    "    query_num=processing_config[\"query_num\"],\n",
    "    topk=processing_config[\"topk\"],\n",
    "    mode=\"argmax\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_graph_path = \"/mnt/hdfs/foundation/longlin.kylin/mmagent/data/mems/CZ_1/-UhacNNM_HU_30_5_-1_10_20_0.3_0.6.pkl\"\n",
    "video_graph = load_video_graph(video_graph_path)\n",
    "video_graph.visualize()\n",
    "\n",
    "# print all episodic and semantic nodes\n",
    "for node_id in video_graph.text_nodes:\n",
    "    print(video_graph.nodes[node_id].metadata[\"contents\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "from videograph import VideoGraph\n",
    "from utils.general import *\n",
    "from utils.video_processing import *\n",
    "from utils.chat_api import *\n",
    "from prompts import *\n",
    "\n",
    "from face_processing import process_faces\n",
    "from voice_processing import process_voices\n",
    "from memory_processing import (\n",
    "    process_captions,\n",
    "    generate_captions_and_thinkings_with_ids,\n",
    ")\n",
    "from generate_memory import streaming_process_video\n",
    "\n",
    "processing_config = json.load(open(\"configs/processing_config.json\"))\n",
    "memory_config = json.load(open(\"configs/memory_config.json\"))\n",
    "\n",
    "video_paths = [\n",
    "    \"/mnt/hdfs/foundation/longlin.kylin/mmagent/data/video_clips/PnvZZwlN2yk\"\n",
    "]\n",
    "\n",
    "save_dir = processing_config[\"save_dir\"]\n",
    "generated_memories = os.listdir(save_dir)\n",
    "generated_memories = [\n",
    "    generated_memory\n",
    "    for generated_memory in generated_memories\n",
    "    if generated_memory.endswith(\".pkl\")\n",
    "]\n",
    "video_paths = [\n",
    "    video_path\n",
    "    for video_path in video_paths\n",
    "    if generate_file_name(video_path) + \".pkl\" not in generated_memories\n",
    "]\n",
    "\n",
    "# save_dir = processing_config[\"save_dir\"]\n",
    "# video_paths = ['/mnt/hdfs/foundation/longlin.kylin/mmagent/data/video_clips/EodRBU-HVEI']\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "max_workers = min(cpu_count, processing_config.get(\"max_parallel_videos\", 4))\n",
    "\n",
    "print(f\"Using {max_workers} processes (CPU cores: {cpu_count})\")\n",
    "\n",
    "preprocessing = []\n",
    "\n",
    "\n",
    "def process_single_video(video_path):\n",
    "    video_graph = VideoGraph(**memory_config)\n",
    "    try:\n",
    "        streaming_process_video(video_graph, video_path, preprocessing=preprocessing)\n",
    "    except Exception as e:\n",
    "        log_dir = processing_config[\"log_dir\"]\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        with open(os.path.join(log_dir, f\"generate_memory_error.log\"), \"a\") as f:\n",
    "            f.write(f\"Error processing video {video_path}: {e}\\n\")\n",
    "\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    list(\n",
    "        tqdm(\n",
    "            executor.map(process_single_video, video_paths),\n",
    "            total=len(video_paths),\n",
    "            desc=\"Processing videos\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video graph from /mnt/hdfs/foundation/longlin.kylin/mmagent/data/mems/CZ_1/-UhacNNM_HU_30_5_-1_10_20_0.3_0.6.pkl\n",
      "Generating action 0 times\n",
      "The provided knowledge does not contain any information about the number of dance performances in the video. To answer this question, we need specific details about the occurrences of dance performances, such as their count or descriptions of scenes involving dancing. Since this information is missing, I will generate queries to retrieve the necessary details.\n",
      "\n",
      "[SEARCH]  \n",
      "[  \n",
      "\t\"Total number of dance performances in the video.\",  \n",
      "\t\"Descriptions of scenes involving dance performances.\",  \n",
      "\t\"How many distinct dance performances are shown?\",  \n",
      "\t\"CLIP_1\",  \n",
      "\t\"CLIP_2\",  \n",
      "\t\"CLIP_3\"  \n",
      "]  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "parse_video_caption() missing 1 required positional argument: 'video_caption'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m video_graph \u001b[39m=\u001b[39m load_video_graph(video_graph_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHow many dance performances are there in total in the video?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m answer \u001b[39m=\u001b[39m answer_with_retrieval(video_graph, question)\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/retrieve.py:299\u001b[0m, in \u001b[0;36manswer_with_retrieval\u001b[0;34m(video_graph, question, query_num, topk, auto_refresh, mode)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39melif\u001b[39;00m action_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msearch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 299\u001b[0m     new_clips \u001b[39m=\u001b[39m retrieve_from_videograph(video_graph, action_content, topk, mode)\n\u001b[1;32m    300\u001b[0m     new_clips \u001b[39m=\u001b[39m [new_clip \u001b[39mfor\u001b[39;00m new_clip \u001b[39min\u001b[39;00m new_clips \u001b[39mif\u001b[39;00m new_clip \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m related_clips]\n\u001b[1;32m    301\u001b[0m     new_memories \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/retrieve.py:137\u001b[0m, in \u001b[0;36mretrieve_from_videograph\u001b[0;34m(video_graph, queries_original, topk, mode)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mretrieve_from_videograph\u001b[39m(video_graph, queries_original, topk\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 137\u001b[0m     queries \u001b[39m=\u001b[39m back_translate(video_graph, queries_original)\n\u001b[1;32m    138\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQueries: \u001b[39m\u001b[39m{\u001b[39;00mqueries\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-embedding-3-large\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/retrieve.py:28\u001b[0m, in \u001b[0;36mback_translate\u001b[0;34m(video_graph, queries)\u001b[0m\n\u001b[1;32m     26\u001b[0m translated_queries \u001b[39m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i, query \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(queries):\n\u001b[0;32m---> 28\u001b[0m     entities \u001b[39m=\u001b[39m parse_video_caption(query)\n\u001b[1;32m     29\u001b[0m     to_be_translated \u001b[39m=\u001b[39m [query]\n\u001b[1;32m     30\u001b[0m     \u001b[39mfor\u001b[39;00m entity \u001b[39min\u001b[39;00m entities:\n",
      "\u001b[0;31mTypeError\u001b[0m: parse_video_caption() missing 1 required positional argument: 'video_caption'"
     ]
    }
   ],
   "source": [
    "video_graph_path = \"/mnt/hdfs/foundation/longlin.kylin/mmagent/data/mems/CZ_1/-UhacNNM_HU_30_5_-1_10_20_0.3_0.6.pkl\"\n",
    "video_graph = load_video_graph(video_graph_path)\n",
    "\n",
    "question = \"How many dance performances are there in total in the video?\"\n",
    "answer = answer_with_retrieval(video_graph, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "9e153c75-6630-46b6-af1a-76d1a8277f1c",
  "filePath": "/mnt/bn/videonasi18n/longlin.kylin/tce-face-extraction/demo.ipynb",
  "kernelspec": {
   "display_name": "vlm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
