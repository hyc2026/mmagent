{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bytedtrace/__init__.py:108: UserWarning: [bytedtrace] global tracer is already initialized.\n",
      "  warnings.warn('[bytedtrace] global tracer is already initialized.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "from videograph import VideoGraph\n",
    "from utils.general import *\n",
    "from utils.video_processing import *\n",
    "from utils.chat_api import *\n",
    "from prompts import *\n",
    "\n",
    "from face_processing import process_faces\n",
    "from voice_processing import process_voices\n",
    "from memory_processing import (\n",
    "    process_captions,\n",
    "    generate_captions_and_thinkings_with_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(video_graph, base64_video, base64_frames, base64_audio):\n",
    "\n",
    "    id2voices = process_voices(video_graph, base64_audio, base64_video)\n",
    "    print(\"Finish processing voices\")\n",
    "\n",
    "    print(f\"processing {len(base64_frames)} frames...\")\n",
    "\n",
    "    id2faces = process_faces(video_graph, base64_frames)\n",
    "    # print(id2faces.keys())\n",
    "    print(\"Finish processing faces\")\n",
    "\n",
    "    episodic_captions, semantic_captions = generate_captions_and_thinkings_with_ids(\n",
    "        video_graph,\n",
    "        base64_video,\n",
    "        base64_frames,\n",
    "        base64_audio,\n",
    "        id2faces,\n",
    "        id2voices,\n",
    "    )\n",
    "\n",
    "    process_captions(video_graph, episodic_captions, type=\"episodic\")\n",
    "    process_captions(video_graph, semantic_captions, type=\"semantic\")\n",
    "\n",
    "    print(\"Finish processing segment\")\n",
    "\n",
    "\n",
    "def streaming_process_video(\n",
    "    video_graph, video_path, interval_seconds, fps, segment_limit=None\n",
    "):\n",
    "    \"\"\"Process video segments at specified intervals with given fps.\n",
    "\n",
    "    Args:\n",
    "        video_graph (VideoGraph): Graph object to store video information\n",
    "        video_path (str): Path to the video file or directory containing clips\n",
    "        interval_seconds (float): Time interval between segments in seconds\n",
    "        fps (float): Frames per second to extract from each segment\n",
    "\n",
    "    Returns:\n",
    "        None: Updates video_graph in place with processed segments\n",
    "    \"\"\"\n",
    "    if os.path.isfile(video_path):\n",
    "        # Process single video file\n",
    "        video_info = get_video_info(video_path)\n",
    "        print(video_info)\n",
    "\n",
    "        # Process each interval\n",
    "        count = 0\n",
    "        for start_time in np.arange(0, video_info[\"duration\"], interval_seconds):\n",
    "            if start_time + interval_seconds > video_info[\"duration\"]:\n",
    "                break\n",
    "\n",
    "            print(\"=\" * 20)\n",
    "            count += 1\n",
    "\n",
    "            print(f\"Loading {count}-th clip starting at {start_time} seconds...\")\n",
    "            base64_video, base64_frames, base64_audio = process_video_clip(\n",
    "                video_path, start_time, interval_seconds, fps, audio_format=\"wav\"\n",
    "            )\n",
    "\n",
    "            # check dtype\n",
    "            # print(type(base64_video), type(base64_frames[0]), type(base64_audio))\n",
    "\n",
    "            # Process frames for this interval\n",
    "            if base64_frames:\n",
    "                print(\n",
    "                    f\"Starting processing {count}-th clip starting at {start_time} seconds...\"\n",
    "                )\n",
    "                process_segment(\n",
    "                    video_graph,\n",
    "                    base64_video,\n",
    "                    base64_frames,\n",
    "                    base64_audio,\n",
    "                )\n",
    "\n",
    "            if segment_limit is not None and count >= segment_limit:\n",
    "                break\n",
    "\n",
    "    elif os.path.isdir(video_path):\n",
    "        # Process directory of numbered clips\n",
    "        files = os.listdir(video_path)\n",
    "        # Filter for video files and sort by numeric value in filename\n",
    "        video_files = [\n",
    "            f for f in files if any(f.endswith(ext) for ext in [\".mp4\", \".avi\", \".mov\"])\n",
    "        ]\n",
    "        video_files.sort(key=lambda x: int(\"\".join(filter(str.isdigit, x))))\n",
    "\n",
    "        for count, video_file in enumerate(video_files, 1):\n",
    "            print(\"=\" * 20)\n",
    "            full_path = os.path.join(video_path, video_file)\n",
    "            print(f\"Processing clip {count}: {full_path}\")\n",
    "\n",
    "            base64_video, base64_frames, base64_audio = process_video_clip(\n",
    "                full_path, 0, None, fps, audio_format=\"wav\"\n",
    "            )\n",
    "\n",
    "            if base64_frames:\n",
    "                process_segment(\n",
    "                    video_graph,\n",
    "                    base64_video,\n",
    "                    base64_frames,\n",
    "                    base64_audio,\n",
    "                )\n",
    "\n",
    "            if segment_limit is not None and count >= segment_limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_config = json.load(open(\"configs/processing_config.json\"))\n",
    "memory_config = json.load(open(\"configs/memory_config.json\"))\n",
    "# video paths can be paths to directories or paths to mp4 files\n",
    "video_paths = processing_config[\"video_paths\"]\n",
    "\n",
    "for video_path in video_paths:\n",
    "\n",
    "    video_graph = VideoGraph(**memory_config)\n",
    "\n",
    "    streaming_process_video(\n",
    "        video_graph,\n",
    "        video_path,\n",
    "        processing_config[\"interval_seconds\"],\n",
    "        processing_config[\"fps\"],\n",
    "        processing_config[\"segment_limit\"],\n",
    "    )\n",
    "\n",
    "    save_dir = \"data/video_graphs\"\n",
    "    save_video_graph(\n",
    "        video_graph, video_path, save_dir, (processing_config, memory_config)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video graph from data/video_graphs/5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75.pkl\n",
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 15:15:48,339 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 15:15:49,182 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating equivalences 0 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 15:15:50,580 - httpx - INFO - HTTP Request: POST https://search-va.byteintl.net/gpt/openapi/online/v2/crawl/openai/deployments/gpt-4o-2024-11-20/chat/completions?api-version=2024-03-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m video_graph \u001b[39m=\u001b[39m load_video_graph(video_graph_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# for text_node in video_graph.text_nodes:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#     print(video_graph.nodes[text_node].metadata['contents'])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# for nodes, weight in video_graph.edges.items():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#            print(video_graph.nodes[nodes[1]].metadata['contents'])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#         print(weight)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m equivalences \u001b[39m=\u001b[39m video_graph\u001b[39m.\u001b[39;49mextract_equivalences()\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(equivalences)\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# video_graph.summarize(logging=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# save_dir = \"data/video_graphs\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# save_video_graph(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#     video_graph, None, save_dir, None, file_name='5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75_augmented.pkl'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/videonasi18n/longlin.kylin/mmagent/demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# video_graph.visualize()\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/videograph.py:377\u001b[0m, in \u001b[0;36mVideoGraph.extract_equivalences\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[39m# Get filtered semantic nodes and their contents\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m filtered_semantic_nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfix_collisions(node_id, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdropout\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    378\u001b[0m filtered_semantics \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodes[filtered_semantic_node]\u001b[39m.\u001b[39mmetadata[\u001b[39m'\u001b[39m\u001b[39mcontents\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m] \n\u001b[1;32m    379\u001b[0m                     \u001b[39mfor\u001b[39;00m filtered_semantic_node \u001b[39min\u001b[39;00m filtered_semantic_nodes]\n\u001b[1;32m    381\u001b[0m \u001b[39m# Generate equivalences using LLM\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/videograph.py:314\u001b[0m, in \u001b[0;36mVideoGraph.fix_collisions\u001b[0;34m(self, node_id, mode)\u001b[0m\n\u001b[1;32m    311\u001b[0m connected_nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_connected_nodes(node_id, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39msemantic\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    313\u001b[0m \u001b[39m# cluster the connected nodes\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m clusters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cluster_semantic_nodes(connected_nodes)\n\u001b[1;32m    316\u001b[0m cluster_ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(clusters))\n\u001b[1;32m    317\u001b[0m cluster_ids\u001b[39m.\u001b[39msort()\n",
      "File \u001b[0;32m/mnt/bn/videonasi18n/longlin.kylin/mmagent/videograph.py:78\u001b[0m, in \u001b[0;36mVideoGraph._cluster_semantic_nodes\u001b[0;34m(self, nodes, threshold)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cluster_semantic_nodes\u001b[39m(\u001b[39mself\u001b[39m, nodes, threshold\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m):\n\u001b[1;32m     72\u001b[0m     \u001b[39m# cluster the nodes using cosine similarity\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[39m# return a list of clusters, each cluster is a list of node ids\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[39m# each node id is a string\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \n\u001b[1;32m     76\u001b[0m     \u001b[39m# calculate pairwise cosine similarities between all nodes\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodes[node_id]\u001b[39m.\u001b[39membeddings[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m node_id \u001b[39min\u001b[39;00m nodes]\n\u001b[0;32m---> 78\u001b[0m     similarities \u001b[39m=\u001b[39m cosine_similarity(embeddings)\n\u001b[1;32m     80\u001b[0m     \u001b[39m# Convert similarity matrix to distance matrix\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[39m# For cosine similarity s, distance = 1 - s\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[39m# This ensures that:\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[39m# - Similar vectors (s close to 1) have small distance\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[39m# - Dissimilar vectors (s close to 0) have large distance\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     distances \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m similarities\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \n\u001b[1;32m   1697\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1737\u001b[0m \u001b[39m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1741\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m   1743\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/pairwise.py:190\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[1;32m    187\u001b[0m     dtype \u001b[39m=\u001b[39m dtype_float\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m Y \u001b[39mis\u001b[39;00m X \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     X \u001b[39m=\u001b[39m Y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    193\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    194\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    195\u001b[0m         ensure_all_finite\u001b[39m=\u001b[39;49mensure_all_finite,\n\u001b[1;32m    196\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    197\u001b[0m         ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m         ensure_2d\u001b[39m=\u001b[39mensure_2d,\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1093\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1087\u001b[0m             msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   1088\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{\u001b[39;00marray\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1091\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1092\u001b[0m             )\n\u001b[0;32m-> 1093\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(array\u001b[39m.\u001b[39mdtype, \u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1096\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1097\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1099\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "video_graph_path = \"data/video_graphs/5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75.pkl\"\n",
    "video_graph = load_video_graph(video_graph_path)\n",
    "# for text_node in video_graph.text_nodes:\n",
    "#     print(video_graph.nodes[text_node].metadata['contents'])\n",
    "# for nodes, weight in video_graph.edges.items():\n",
    "#     if weight > 1:\n",
    "#         if video_graph.nodes[nodes[0]].type in [\"episodic\", \"semantic\"]:\n",
    "#            print(video_graph.nodes[nodes[0]].metadata['contents'])\n",
    "#         else:\n",
    "#            print(video_graph.nodes[nodes[1]].metadata['contents'])\n",
    "#         print(weight)\n",
    "\n",
    "equivalences = video_graph.extract_equivalences()\n",
    "print(equivalences)\n",
    "           \n",
    "# video_graph.summarize(logging=True)\n",
    "# save_dir = \"data/video_graphs\"\n",
    "# save_video_graph(\n",
    "#     video_graph, None, save_dir, None, file_name='5-Poor-People-vs-1-Secret-Millionaire_60_5_5_10_20_0.3_0.6_0.75_augmented.pkl'\n",
    "# )\n",
    "# video_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from retrieve import retrieve_from_videograph\n",
    "# from videograph import VideoGraph\n",
    "# from utils.chat_api import (\n",
    "#     generate_messages,\n",
    "#     get_response_with_retry,\n",
    "#     parallel_get_embedding,\n",
    "# )\n",
    "# from utils.general import validate_and_fix_python_list\n",
    "# from prompts import prompt_memory_retrieval\n",
    "\n",
    "# MAX_RETRIES = 3\n",
    "\n",
    "\n",
    "# def generate_queries(question, existing_knowledge=None, query_num=1):\n",
    "#     input = [\n",
    "#         {\n",
    "#             \"type\": \"text\",\n",
    "#             \"content\": prompt_memory_retrieval.format(\n",
    "#                 question=question,\n",
    "#                 query_num=query_num,\n",
    "#                 existing_knowledge=existing_knowledge,\n",
    "#             ),\n",
    "#         }\n",
    "#     ]\n",
    "#     messages = generate_messages(input)\n",
    "#     model = \"gpt-4o-2024-11-20\"\n",
    "#     queries = None\n",
    "#     for i in range(MAX_RETRIES):\n",
    "#         print(f\"Generating queries {i} times\")\n",
    "#         queries = get_response_with_retry(model, messages)[0]\n",
    "#         queries = validate_and_fix_python_list(queries)\n",
    "#         if queries is not None:\n",
    "#             break\n",
    "#     if queries is None:\n",
    "#         raise Exception(\"Failed to generate queries\")\n",
    "#     return queries\n",
    "\n",
    "\n",
    "# def retrieve_from_videograph(videograph, question, topk=3):\n",
    "#     queries = generate_queries(question)\n",
    "#     print(f\"Queries: {queries}\")\n",
    "\n",
    "#     model = \"text-embedding-3-large\"\n",
    "#     query_embeddings = parallel_get_embedding(model, queries)[0]\n",
    "\n",
    "#     related_nodes = []\n",
    "\n",
    "#     for query_embedding in query_embeddings:\n",
    "#         nodes = videograph.search_text_nodes(query_embedding)\n",
    "#         related_nodes.extend(nodes)\n",
    "\n",
    "#     related_nodes = list(set(related_nodes))\n",
    "#     return related_nodes\n",
    "\n",
    "\n",
    "# question = \"Denny\"\n",
    "# retrieved_nodes = retrieve_from_videograph(video_graph, question)\n",
    "# print(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "9e153c75-6630-46b6-af1a-76d1a8277f1c",
  "filePath": "/mnt/bn/videonasi18n/longlin.kylin/tce-face-extraction/demo.ipynb",
  "kernelspec": {
   "display_name": "vlm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
