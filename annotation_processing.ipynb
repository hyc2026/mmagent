{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 100 videos from CZ_3\n",
      "Detected 60 videos from CZ_4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "csv_paths = [\"data/annotations/raw/CZ_3.csv\", \"data/annotations/raw/CZ_4.csv\"]\n",
    "for csv_path in csv_paths:\n",
    "    marker = csv_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # modify the column name\n",
    "    for col in df.columns:\n",
    "        df = df.rename(columns={col: col.strip().strip(\"\\\\n\")})\n",
    "\n",
    "    # replace nan with \"\"\n",
    "    df = df.replace(np.nan, \"\")\n",
    "\n",
    "    video = None\n",
    "    video_list = []\n",
    "\n",
    "    for row in df.iterrows():\n",
    "        row = row[1]\n",
    "        if row[\"视频序号\"]:\n",
    "            if video:\n",
    "                video_list.append(video)\n",
    "            video = {}\n",
    "            video[\"video_id\"] = marker + \"_\" + str(int(row[\"视频序号\"]))\n",
    "            video[\"video_url\"] = row[\"视频_url\"]\n",
    "            video[\"video_duration\"] = row[\"视频长度（min）\"]\n",
    "            video[\"video_type\"] = row[\"视频类型\"]\n",
    "            video[\"qa_list\"] = []\n",
    "        qa = {\n",
    "            \"question\": row[\"问题\"],\n",
    "            \"answer\": row[\"答案\"],\n",
    "            \"question_type\": row[\"问题类型\"],\n",
    "            \"knowledge\": row[\"若涉及通用知识推理，则写出推理步骤（知识点）\"],\n",
    "            \"reasoning\": row[\"依据（解题思路）\"],\n",
    "        }\n",
    "        video[\"qa_list\"].append(qa)\n",
    "\n",
    "    video_list.append(video)\n",
    "\n",
    "    print(f\"Detected {len(video_list)} videos from {marker}\")\n",
    "\n",
    "    # save the video_list to a json file\n",
    "    with open(f\"data/annotations/raw/{marker}.json\", \"w\") as f:\n",
    "        json.dump(video_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error video urls: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "processing_config = json.load(open(\"configs/processing_config.json\"))\n",
    "log_dir = processing_config[\"log_dir\"]\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "annotaions_paths = [\n",
    "    \"data/annotations/raw/CZ_3.json\",\n",
    "    \"data/annotations/raw/CZ_4.json\"\n",
    "]\n",
    "\n",
    "for annotaions_path in annotaions_paths:\n",
    "    marker = annotaions_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    with open(annotaions_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for video in data:\n",
    "        url = video[\"video_url\"]\n",
    "        if not url.startswith(\"https://www.youtube.com/watch?v=\"):\n",
    "            count += 1\n",
    "            with open(\n",
    "                os.path.join(log_dir, f\"annotation_processing_error.log\"), \"a\"\n",
    "            ) as f:\n",
    "                f.write(f\"Error video url from {marker}: {url}\" + \"\\n\")\n",
    "\n",
    "print(f\"Total error video urls: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:58<00:00, 29.44s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils.chat_api import generate_messages, parallel_get_response\n",
    "from prompts import prompt_refine_qa_list\n",
    "from utils.general import validate_and_fix_python_list\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "annotaions_paths = [\n",
    "    \"data/annotations/raw/CZ_3.json\",\n",
    "    \"data/annotations/raw/CZ_4.json\"\n",
    "]\n",
    "\n",
    "for annotaions_path in tqdm(annotaions_paths):\n",
    "    marker = annotaions_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    with open(annotaions_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    inputs = []\n",
    "\n",
    "    for video in data:\n",
    "        qa_list = video[\"qa_list\"]\n",
    "        qa_list = [\n",
    "            {\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answer\": qa[\"answer\"],\n",
    "                \"reasoning\": qa[\"reasoning\"],\n",
    "            }\n",
    "            for qa in qa_list\n",
    "        ]\n",
    "        input = [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"content\": prompt_refine_qa_list.format(qa_list=qa_list),\n",
    "            }\n",
    "        ]\n",
    "        inputs.append(input)\n",
    "\n",
    "    messages = [generate_messages(input) for input in inputs]\n",
    "    model = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "    responses = parallel_get_response(model, messages)[0]\n",
    "\n",
    "    for video, response in zip(data, responses):\n",
    "        translated_qa_list = validate_and_fix_python_list(response)\n",
    "        for qa, translated_qa in zip(video[\"qa_list\"], translated_qa_list):\n",
    "            qa[\"question\"] = translated_qa[\"question\"]\n",
    "            qa[\"answer\"] = translated_qa[\"answer\"]\n",
    "            qa[\"reasoning\"] = translated_qa[\"reasoning\"]\n",
    "\n",
    "    with open(f\"data/annotations/raw/{marker}_refined.json\", \"w\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 419430.40it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 258641.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_url(url):\n",
    "    if not url.startswith(\"https://www.youtube.com/\"):\n",
    "        return None\n",
    "    if \"&\" in url:\n",
    "        pruned_url = url.split(\"&\")[0]\n",
    "    else:\n",
    "        pruned_url = url\n",
    "    return pruned_url, pruned_url.split(\"watch?v=\")[1]\n",
    "\n",
    "annotations_paths = [\n",
    "    \"data/annotations/raw/CZ_3_refined.json\",\n",
    "    \"data/annotations/raw/CZ_4_refined.json\"\n",
    "]\n",
    "base_save_dir = \"/mnt/hdfs/foundation/longlin.kylin/mmagent/data/raw_videos\"\n",
    "\n",
    "error_count = 0\n",
    "\n",
    "for annotations_path in annotations_paths:\n",
    "    marker = annotations_path.split(\"/\")[-1].split(\".\")[0].strip(\"_refined\")\n",
    "    target_dir = marker + \"/\"\n",
    "    save_dir = os.path.join(base_save_dir, marker)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(annotations_path, \"r\") as f:\n",
    "        videos = json.load(f)\n",
    "\n",
    "    for video in tqdm(videos):\n",
    "        url = video[\"video_url\"]\n",
    "        if not url.startswith(\"https://www.youtube.com/watch?v=\"):\n",
    "            print(url)\n",
    "            video[\"path\"] = \"\"\n",
    "            error_count += 1\n",
    "            continue\n",
    "        pruned_url, video_id = parse_url(url)\n",
    "        save_file = os.path.join(save_dir, video_id + \".mp4\")\n",
    "        video[\"path\"] = save_file\n",
    "        video[\"video_url\"] = pruned_url\n",
    "    \n",
    "    with open(annotations_path, \"w\") as f:\n",
    "        json.dump(videos, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(error_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "small_test = []\n",
    "samples_per_file = 50\n",
    "with open(\"data/annotations/raw/CZ_1_refined.json\", \"r\") as f:\n",
    "    CZ = json.load(f)\n",
    "with open(\"data/annotations/raw/ZZ_1_refined.json\", \"r\") as f:\n",
    "    ZZ = json.load(f)\n",
    "\n",
    "count = 0\n",
    "for video in CZ:\n",
    "    if not video[\"video_url\"].startswith(\"https://www.youtube.com/watch?v=\"):\n",
    "        continue\n",
    "    if os.path.exists(video[\"path\"]):\n",
    "        small_test.append(video)\n",
    "        count += 1\n",
    "        if count >= samples_per_file:\n",
    "            break\n",
    "\n",
    "count = 0\n",
    "for video in ZZ:\n",
    "    if not video[\"video_url\"].startswith(\"https://www.youtube.com/watch?v=\"):\n",
    "        continue\n",
    "    if os.path.exists(video[\"path\"]):\n",
    "        small_test.append(video)\n",
    "        count += 1\n",
    "        if count >= samples_per_file:\n",
    "            break\n",
    "\n",
    "with open(\"data/annotations/small_test.json\", \"w\") as f:\n",
    "    json.dump(small_test, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_list = [\n",
    "    \"data/annotations/raw/CZ_1_refined.json\",\n",
    "    \"data/annotations/raw/CZ_2_refined.json\",\n",
    "    \"data/annotations/raw/CZ_3_refined.json\",\n",
    "    \"data/annotations/raw/ZZ_1_refined.json\",\n",
    "    \"data/annotations/raw/ZZ_2_refined.json\",\n",
    "    \"data/annotations/raw/ZZ_3_refined.json\",\n",
    "    \"data/annotations/raw/ZZ_4_refined.json\", \n",
    "]\n",
    "test_set = json.load(open(\"data/annotations/small_test.json\", \"r\"))\n",
    "test_id = [video[\"video_id\"] for video in test_set]\n",
    "small_training_set = []\n",
    "for data in data_list:\n",
    "    with open(data, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    for video in data:\n",
    "        if video[\"video_id\"] not in test_id:\n",
    "            small_training_set.append(video)\n",
    "\n",
    "print(len(test_id))\n",
    "print(len(small_training_set))\n",
    "with open(\"data/annotations/small_train.json\", \"w\") as f:\n",
    "    json.dump(small_training_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.video_processing import get_video_info\n",
    "\n",
    "with open(\"data/annotations/small_train.json\", \"r\") as f:\n",
    "    videos = json.load(f)\n",
    "\n",
    "filtered_videos = []\n",
    "filtered_video_ids = []\n",
    "\n",
    "for video in videos:\n",
    "    try:\n",
    "        video_info = get_video_info(video[\"path\"])\n",
    "        video_id = video[\"path\"].split(\"&\")[0].split(\"watch?v=\")[-1]\n",
    "        if video_info[\"height\"] == 720 and video_id not in filtered_video_ids:\n",
    "            filtered_videos.append(video)\n",
    "            filtered_video_ids.append(video_id)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(len(filtered_videos))\n",
    "with open(\"data/annotations/train_500.json\", \"w\") as f:\n",
    "    json.dump(filtered_videos[:500], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.general import *\n",
    "\n",
    "processing_config = json.load(open(\"configs/processing_config.json\"))\n",
    "save_dir = os.path.join(processing_config[\"save_dir\"])\n",
    "clip_dir = processing_config[\"input_dir\"]\n",
    "\n",
    "\n",
    "def add_paths(data):\n",
    "    with open(data, \"r\") as f:\n",
    "        videos = json.load(f)\n",
    "    for video in videos:\n",
    "        video_id = video[\"path\"].split(\"/\")[-1].split(\".\")[0]\n",
    "        marker = video[\"video_id\"][:4]\n",
    "        save_path = os.path.join(\n",
    "            save_dir, marker, generate_file_name(video[\"path\"]) + \".pkl\"\n",
    "        )\n",
    "        clip_path = os.path.join(clip_dir, marker, video_id)\n",
    "        if os.path.exists(save_path):\n",
    "            video[\"mem_path\"] = save_path\n",
    "        else:\n",
    "            video[\"mem_path\"] = \"\"\n",
    "        if os.path.exists(clip_path):\n",
    "            video[\"clip_path\"] = clip_path\n",
    "        else:\n",
    "            video[\"clip_path\"] = \"\"\n",
    "    with open(data, \"w\") as f:\n",
    "        json.dump(videos, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "add_paths(\"data/annotations/small_test.json\")\n",
    "add_paths(\"data/annotations/small_train.json\")\n",
    "add_paths(\"data/annotations/train_500.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def convert_to_jsonl(data):\n",
    "    with open(data, \"r\") as f:\n",
    "        videos = json.load(f)\n",
    "    qas = []\n",
    "    for video in videos:\n",
    "        for qa in video[\"qa_list\"]:\n",
    "            qas.append(\n",
    "                {\n",
    "                    \"video_id\": video[\"video_id\"],\n",
    "                    \"video_url\": video[\"video_url\"],\n",
    "                    \"video_path\": video[\"path\"],\n",
    "                    \"clip_path\": video[\"clip_path\"],\n",
    "                    \"mem_path\": video[\"mem_path\"],\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answer\": qa[\"answer\"],\n",
    "                    \"reasoning\": qa[\"reasoning\"]\n",
    "                }\n",
    "            )\n",
    "    with open(f\"data/annotations/{data.split('/')[-1].split('.')[0]}.jsonl\", \"w\") as f:\n",
    "        for qa in qas:\n",
    "            f.write(json.dumps(qa) + \"\\n\")\n",
    "\n",
    "\n",
    "convert_to_jsonl(\"data/annotations/small_test.json\")\n",
    "convert_to_jsonl(\"data/annotations/small_train.json\")\n",
    "convert_to_jsonl(\"data/annotations/train_500.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_contains_mem(data):\n",
    "    count = 0\n",
    "    with open(data, \"r\") as f:\n",
    "        videos = json.load(f)\n",
    "    for video in videos:\n",
    "        if video[\"mem_path\"]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "print(check_if_contains_mem(\"data/annotations/small_train.json\"))\n",
    "print(check_if_contains_mem(\"data/annotations/small_test.json\"))\n",
    "print(check_if_contains_mem(\"data/annotations/train_500.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filtered_questions = []\n",
    "\n",
    "with open(\n",
    "    \"/mnt/bn/videonasi18n/longlin.kylin/mmagent/data/annotations/results/0416/baseline_blindly_answers_verified.jsonl\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        sample = json.loads(line)\n",
    "        if sample[\"verify_result\"].lower().startswith(\"yes\"):\n",
    "            filtered_questions.append(i)\n",
    "\n",
    "with open(\n",
    "    \"/mnt/bn/videonasi18n/longlin.kylin/mmagent/data/annotations/results/0417/verified_small_test.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    samples = json.load(f)\n",
    "    for i, sample in enumerate(samples):\n",
    "        if sample[\"verify_result\"].lower().startswith(\"yes\"):\n",
    "            filtered_questions.append(i)\n",
    "\n",
    "filtered_questions = list(set(filtered_questions))\n",
    "\n",
    "print(len(filtered_questions))\n",
    "\n",
    "data_with_flags = []\n",
    "with open(\"data/annotations/small_test.jsonl\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        sample = json.loads(line)\n",
    "        if i in filtered_questions:\n",
    "            sample[\"flag\"] = True\n",
    "        else:\n",
    "            sample[\"flag\"] = False\n",
    "        data_with_flags.append(sample)\n",
    "\n",
    "with open(\"data/annotations/small_test.jsonl\", \"w\") as f:\n",
    "    for sample in data_with_flags:\n",
    "        f.write(json.dumps(sample) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "64ede5b9-028e-46b8-a027-d3d3a2f1a4f2",
  "filePath": "/mnt/bn/videonasi18n/longlin.kylin/mmagent/annotation_processing.ipynb",
  "kernelspec": {
   "display_name": "vlm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
